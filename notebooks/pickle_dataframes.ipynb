{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('voter-fraud': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a0f90dcbb54bea4e60f894f8fd1d686cc0f74395b4029405cc9f13e0b975e641"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve paths from root project directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"16-dec\"\n",
    "DATA_DIR = \"../data/{}/\".format(DATE)\n",
    "EXPORT_DIR = \"../data/dataframes/{}/\".format(DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.dataframes import create_tweet_df, create_user_df, create_retweet_df, aggregate_counts_by_hour, aggregate_most_common_hashtags, create_media_df\n",
    "from data_tools import load_crawled_terms "
   ]
  },
  {
   "source": [
    "## Load from cache"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweet_df = pd.read_pickle(EXPORT_DIR + 'df_retweets.pickle')\n",
    "# df_users = pd.read_pickle(EXPORT_DIR + 'df_users.pickle')\n",
    "recent_tweet_df = pd.read_pickle(EXPORT_DIR + 'df_recent_tweets.pickle')"
   ]
  },
  {
   "source": [
    "## Build from scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWLED_TERMS = load_crawled_terms(\"../keywords-3nov.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 25566698 json lines\n",
      "(0%): 100000 lines in ../data/16-dec/parsed_retweets.json processed (0.34247398376464844 sec)\n",
      "(1%): 200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3321700096130371 sec)\n",
      "(1%): 300000 lines in ../data/16-dec/parsed_retweets.json processed (0.37143874168395996 sec)\n",
      "(2%): 400000 lines in ../data/16-dec/parsed_retweets.json processed (0.33399295806884766 sec)\n",
      "(2%): 500000 lines in ../data/16-dec/parsed_retweets.json processed (0.40375328063964844 sec)\n",
      "(2%): 600000 lines in ../data/16-dec/parsed_retweets.json processed (0.40134692192077637 sec)\n",
      "(3%): 700000 lines in ../data/16-dec/parsed_retweets.json processed (0.39596986770629883 sec)\n",
      "(3%): 800000 lines in ../data/16-dec/parsed_retweets.json processed (0.39369988441467285 sec)\n",
      "(4%): 900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3447408676147461 sec)\n",
      "(4%): 1000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3477671146392822 sec)\n",
      "(4%): 1100000 lines in ../data/16-dec/parsed_retweets.json processed (0.38245415687561035 sec)\n",
      "(5%): 1200000 lines in ../data/16-dec/parsed_retweets.json processed (0.35731983184814453 sec)\n",
      "(5%): 1300000 lines in ../data/16-dec/parsed_retweets.json processed (0.35004687309265137 sec)\n",
      "(5%): 1400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3902091979980469 sec)\n",
      "(6%): 1500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3361999988555908 sec)\n",
      "(6%): 1600000 lines in ../data/16-dec/parsed_retweets.json processed (0.37253713607788086 sec)\n",
      "(7%): 1700000 lines in ../data/16-dec/parsed_retweets.json processed (0.35453200340270996 sec)\n",
      "(7%): 1800000 lines in ../data/16-dec/parsed_retweets.json processed (0.33101606369018555 sec)\n",
      "(7%): 1900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3569941520690918 sec)\n",
      "(8%): 2000000 lines in ../data/16-dec/parsed_retweets.json processed (0.34599804878234863 sec)\n",
      "(8%): 2100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3346900939941406 sec)\n",
      "(9%): 2200000 lines in ../data/16-dec/parsed_retweets.json processed (0.370739221572876 sec)\n",
      "(9%): 2300000 lines in ../data/16-dec/parsed_retweets.json processed (0.33524608612060547 sec)\n",
      "(9%): 2400000 lines in ../data/16-dec/parsed_retweets.json processed (0.32849788665771484 sec)\n",
      "(10%): 2500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3365039825439453 sec)\n",
      "(10%): 2600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3627052307128906 sec)\n",
      "(11%): 2700000 lines in ../data/16-dec/parsed_retweets.json processed (0.32883691787719727 sec)\n",
      "(11%): 2800000 lines in ../data/16-dec/parsed_retweets.json processed (0.38588690757751465 sec)\n",
      "(11%): 2900000 lines in ../data/16-dec/parsed_retweets.json processed (0.33585405349731445 sec)\n",
      "(12%): 3000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3300297260284424 sec)\n",
      "(12%): 3100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3421759605407715 sec)\n",
      "(13%): 3200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3510169982910156 sec)\n",
      "(13%): 3300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3380310535430908 sec)\n",
      "(13%): 3400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3759651184082031 sec)\n",
      "(14%): 3500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3358290195465088 sec)\n",
      "(14%): 3600000 lines in ../data/16-dec/parsed_retweets.json processed (0.32555389404296875 sec)\n",
      "(14%): 3700000 lines in ../data/16-dec/parsed_retweets.json processed (0.38767170906066895 sec)\n",
      "(15%): 3800000 lines in ../data/16-dec/parsed_retweets.json processed (0.365009069442749 sec)\n",
      "(15%): 3900000 lines in ../data/16-dec/parsed_retweets.json processed (0.4512290954589844 sec)\n",
      "(16%): 4000000 lines in ../data/16-dec/parsed_retweets.json processed (0.4154818058013916 sec)\n",
      "(16%): 4100000 lines in ../data/16-dec/parsed_retweets.json processed (0.4756288528442383 sec)\n",
      "(16%): 4200000 lines in ../data/16-dec/parsed_retweets.json processed (0.38549184799194336 sec)\n",
      "(17%): 4300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3593020439147949 sec)\n",
      "(17%): 4400000 lines in ../data/16-dec/parsed_retweets.json processed (0.5019030570983887 sec)\n",
      "(18%): 4500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3396289348602295 sec)\n",
      "(18%): 4600000 lines in ../data/16-dec/parsed_retweets.json processed (0.39646482467651367 sec)\n",
      "(18%): 4700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3622901439666748 sec)\n",
      "(19%): 4800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3686351776123047 sec)\n",
      "(19%): 4900000 lines in ../data/16-dec/parsed_retweets.json processed (0.4762732982635498 sec)\n",
      "(20%): 5000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3703651428222656 sec)\n",
      "(20%): 5100000 lines in ../data/16-dec/parsed_retweets.json processed (0.34649014472961426 sec)\n",
      "(20%): 5200000 lines in ../data/16-dec/parsed_retweets.json processed (0.40033483505249023 sec)\n",
      "(21%): 5300000 lines in ../data/16-dec/parsed_retweets.json processed (0.35343313217163086 sec)\n",
      "(21%): 5400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3829038143157959 sec)\n",
      "(22%): 5500000 lines in ../data/16-dec/parsed_retweets.json processed (0.33939027786254883 sec)\n",
      "(22%): 5600000 lines in ../data/16-dec/parsed_retweets.json processed (0.35026097297668457 sec)\n",
      "(22%): 5700000 lines in ../data/16-dec/parsed_retweets.json processed (0.33753490447998047 sec)\n",
      "(23%): 5800000 lines in ../data/16-dec/parsed_retweets.json processed (0.365980863571167 sec)\n",
      "(23%): 5900000 lines in ../data/16-dec/parsed_retweets.json processed (0.34625697135925293 sec)\n",
      "(23%): 6000000 lines in ../data/16-dec/parsed_retweets.json processed (0.4196279048919678 sec)\n",
      "(24%): 6100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3513510227203369 sec)\n",
      "(24%): 6200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3498702049255371 sec)\n",
      "(25%): 6300000 lines in ../data/16-dec/parsed_retweets.json processed (0.4112508296966553 sec)\n",
      "(25%): 6400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3679959774017334 sec)\n",
      "(25%): 6500000 lines in ../data/16-dec/parsed_retweets.json processed (0.5613791942596436 sec)\n",
      "(26%): 6600000 lines in ../data/16-dec/parsed_retweets.json processed (0.37161970138549805 sec)\n",
      "(26%): 6700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3656620979309082 sec)\n",
      "(27%): 6800000 lines in ../data/16-dec/parsed_retweets.json processed (0.38585996627807617 sec)\n",
      "(27%): 6900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3530423641204834 sec)\n",
      "(27%): 7000000 lines in ../data/16-dec/parsed_retweets.json processed (0.41435694694519043 sec)\n",
      "(28%): 7100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3815577030181885 sec)\n",
      "(28%): 7200000 lines in ../data/16-dec/parsed_retweets.json processed (0.365689754486084 sec)\n",
      "(29%): 7300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3822059631347656 sec)\n",
      "(29%): 7400000 lines in ../data/16-dec/parsed_retweets.json processed (0.36128997802734375 sec)\n",
      "(29%): 7500000 lines in ../data/16-dec/parsed_retweets.json processed (0.4446530342102051 sec)\n",
      "(30%): 7600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3772909641265869 sec)\n",
      "(30%): 7700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3661158084869385 sec)\n",
      "(31%): 7800000 lines in ../data/16-dec/parsed_retweets.json processed (0.39776110649108887 sec)\n",
      "(31%): 7900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3539431095123291 sec)\n",
      "(31%): 8000000 lines in ../data/16-dec/parsed_retweets.json processed (0.4111309051513672 sec)\n",
      "(32%): 8100000 lines in ../data/16-dec/parsed_retweets.json processed (0.36871981620788574 sec)\n",
      "(32%): 8200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3351719379425049 sec)\n",
      "(32%): 8300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3378560543060303 sec)\n",
      "(33%): 8400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3515608310699463 sec)\n",
      "(33%): 8500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3318510055541992 sec)\n",
      "(34%): 8600000 lines in ../data/16-dec/parsed_retweets.json processed (0.400421142578125 sec)\n",
      "(34%): 8700000 lines in ../data/16-dec/parsed_retweets.json processed (0.37016820907592773 sec)\n",
      "(34%): 8800000 lines in ../data/16-dec/parsed_retweets.json processed (0.33289194107055664 sec)\n",
      "(35%): 8900000 lines in ../data/16-dec/parsed_retweets.json processed (0.36782407760620117 sec)\n",
      "(35%): 9000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3856849670410156 sec)\n",
      "(36%): 9100000 lines in ../data/16-dec/parsed_retweets.json processed (0.37968873977661133 sec)\n",
      "(36%): 9200000 lines in ../data/16-dec/parsed_retweets.json processed (0.359332799911499 sec)\n",
      "(36%): 9300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3453691005706787 sec)\n",
      "(37%): 9400000 lines in ../data/16-dec/parsed_retweets.json processed (0.34146881103515625 sec)\n",
      "(37%): 9500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3390212059020996 sec)\n",
      "(38%): 9600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3286740779876709 sec)\n",
      "(38%): 9700000 lines in ../data/16-dec/parsed_retweets.json processed (0.37287116050720215 sec)\n",
      "(38%): 9800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3879058361053467 sec)\n",
      "(39%): 9900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3658297061920166 sec)\n",
      "(39%): 10000000 lines in ../data/16-dec/parsed_retweets.json processed (0.35868072509765625 sec)\n",
      "(40%): 10100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3535909652709961 sec)\n",
      "(40%): 10200000 lines in ../data/16-dec/parsed_retweets.json processed (0.4053969383239746 sec)\n",
      "(40%): 10300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3639230728149414 sec)\n",
      "(41%): 10400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3569638729095459 sec)\n",
      "(41%): 10500000 lines in ../data/16-dec/parsed_retweets.json processed (0.36191511154174805 sec)\n",
      "(41%): 10600000 lines in ../data/16-dec/parsed_retweets.json processed (0.39156103134155273 sec)\n",
      "(42%): 10700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3752131462097168 sec)\n",
      "(42%): 10800000 lines in ../data/16-dec/parsed_retweets.json processed (0.4731109142303467 sec)\n",
      "(43%): 10900000 lines in ../data/16-dec/parsed_retweets.json processed (0.34369468688964844 sec)\n",
      "(43%): 11000000 lines in ../data/16-dec/parsed_retweets.json processed (0.4505198001861572 sec)\n",
      "(43%): 11100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3729531764984131 sec)\n",
      "(44%): 11200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3319249153137207 sec)\n",
      "(44%): 11300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3386411666870117 sec)\n",
      "(45%): 11400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3699498176574707 sec)\n",
      "(45%): 11500000 lines in ../data/16-dec/parsed_retweets.json processed (0.32478880882263184 sec)\n",
      "(45%): 11600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3650796413421631 sec)\n",
      "(46%): 11700000 lines in ../data/16-dec/parsed_retweets.json processed (0.34278392791748047 sec)\n",
      "(46%): 11800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3250088691711426 sec)\n",
      "(47%): 11900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3546030521392822 sec)\n",
      "(47%): 12000000 lines in ../data/16-dec/parsed_retweets.json processed (0.347074031829834 sec)\n",
      "(47%): 12100000 lines in ../data/16-dec/parsed_retweets.json processed (0.31748318672180176 sec)\n",
      "(48%): 12200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3321700096130371 sec)\n",
      "(48%): 12300000 lines in ../data/16-dec/parsed_retweets.json processed (0.36266374588012695 sec)\n",
      "(49%): 12400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3563840389251709 sec)\n",
      "(49%): 12500000 lines in ../data/16-dec/parsed_retweets.json processed (0.4006352424621582 sec)\n",
      "(49%): 12600000 lines in ../data/16-dec/parsed_retweets.json processed (0.33185815811157227 sec)\n",
      "(50%): 12700000 lines in ../data/16-dec/parsed_retweets.json processed (0.33371806144714355 sec)\n",
      "(50%): 12800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3525879383087158 sec)\n",
      "(50%): 12900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3304610252380371 sec)\n",
      "(51%): 13000000 lines in ../data/16-dec/parsed_retweets.json processed (0.367534875869751 sec)\n",
      "(51%): 13100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3407931327819824 sec)\n",
      "(52%): 13200000 lines in ../data/16-dec/parsed_retweets.json processed (0.32498621940612793 sec)\n",
      "(52%): 13300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3248589038848877 sec)\n",
      "(52%): 13400000 lines in ../data/16-dec/parsed_retweets.json processed (0.36035990715026855 sec)\n",
      "(53%): 13500000 lines in ../data/16-dec/parsed_retweets.json processed (0.32206201553344727 sec)\n",
      "(53%): 13600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3585479259490967 sec)\n",
      "(54%): 13700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3258681297302246 sec)\n",
      "(54%): 13800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3164699077606201 sec)\n",
      "(54%): 13900000 lines in ../data/16-dec/parsed_retweets.json processed (0.32367897033691406 sec)\n",
      "(55%): 14000000 lines in ../data/16-dec/parsed_retweets.json processed (0.43856000900268555 sec)\n",
      "(55%): 14100000 lines in ../data/16-dec/parsed_retweets.json processed (0.5076279640197754 sec)\n",
      "(56%): 14200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3912529945373535 sec)\n",
      "(56%): 14300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3696439266204834 sec)\n",
      "(56%): 14400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3672640323638916 sec)\n",
      "(57%): 14500000 lines in ../data/16-dec/parsed_retweets.json processed (0.41440606117248535 sec)\n",
      "(57%): 14600000 lines in ../data/16-dec/parsed_retweets.json processed (0.4310121536254883 sec)\n",
      "(57%): 14700000 lines in ../data/16-dec/parsed_retweets.json processed (0.578700065612793 sec)\n",
      "(58%): 14800000 lines in ../data/16-dec/parsed_retweets.json processed (0.4180169105529785 sec)\n",
      "(58%): 14900000 lines in ../data/16-dec/parsed_retweets.json processed (0.34128403663635254 sec)\n",
      "(59%): 15000000 lines in ../data/16-dec/parsed_retweets.json processed (0.33387112617492676 sec)\n",
      "(59%): 15100000 lines in ../data/16-dec/parsed_retweets.json processed (0.33941125869750977 sec)\n",
      "(59%): 15200000 lines in ../data/16-dec/parsed_retweets.json processed (0.4201488494873047 sec)\n",
      "(60%): 15300000 lines in ../data/16-dec/parsed_retweets.json processed (0.37492799758911133 sec)\n",
      "(60%): 15400000 lines in ../data/16-dec/parsed_retweets.json processed (0.4022238254547119 sec)\n",
      "(61%): 15500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3512730598449707 sec)\n",
      "(61%): 15600000 lines in ../data/16-dec/parsed_retweets.json processed (0.32283878326416016 sec)\n",
      "(61%): 15700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3460578918457031 sec)\n",
      "(62%): 15800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3966817855834961 sec)\n",
      "(62%): 15900000 lines in ../data/16-dec/parsed_retweets.json processed (0.32898902893066406 sec)\n",
      "(63%): 16000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3887929916381836 sec)\n",
      "(63%): 16100000 lines in ../data/16-dec/parsed_retweets.json processed (0.33346986770629883 sec)\n",
      "(63%): 16200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3412001132965088 sec)\n",
      "(64%): 16300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3466029167175293 sec)\n",
      "(64%): 16400000 lines in ../data/16-dec/parsed_retweets.json processed (0.31275415420532227 sec)\n",
      "(65%): 16500000 lines in ../data/16-dec/parsed_retweets.json processed (0.312103271484375 sec)\n",
      "(65%): 16600000 lines in ../data/16-dec/parsed_retweets.json processed (0.3596470355987549 sec)\n",
      "(65%): 16700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3326740264892578 sec)\n",
      "(66%): 16800000 lines in ../data/16-dec/parsed_retweets.json processed (0.39960289001464844 sec)\n",
      "(66%): 16900000 lines in ../data/16-dec/parsed_retweets.json processed (0.32915520668029785 sec)\n",
      "(66%): 17000000 lines in ../data/16-dec/parsed_retweets.json processed (0.32007312774658203 sec)\n",
      "(67%): 17100000 lines in ../data/16-dec/parsed_retweets.json processed (0.31702494621276855 sec)\n",
      "(67%): 17200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3518540859222412 sec)\n",
      "(68%): 17300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3307950496673584 sec)\n",
      "(68%): 17400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3771789073944092 sec)\n",
      "(68%): 17500000 lines in ../data/16-dec/parsed_retweets.json processed (0.35091209411621094 sec)\n",
      "(69%): 17600000 lines in ../data/16-dec/parsed_retweets.json processed (0.33243584632873535 sec)\n",
      "(69%): 17700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3449268341064453 sec)\n",
      "(70%): 17800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3370230197906494 sec)\n",
      "(70%): 17900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3138909339904785 sec)\n",
      "(70%): 18000000 lines in ../data/16-dec/parsed_retweets.json processed (0.37706804275512695 sec)\n",
      "(71%): 18100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3485109806060791 sec)\n",
      "(71%): 18200000 lines in ../data/16-dec/parsed_retweets.json processed (0.32090115547180176 sec)\n",
      "(72%): 18300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3383491039276123 sec)\n",
      "(72%): 18400000 lines in ../data/16-dec/parsed_retweets.json processed (0.3893289566040039 sec)\n",
      "(72%): 18500000 lines in ../data/16-dec/parsed_retweets.json processed (0.32700085639953613 sec)\n",
      "(73%): 18600000 lines in ../data/16-dec/parsed_retweets.json processed (0.38495707511901855 sec)\n",
      "(73%): 18700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3366389274597168 sec)\n",
      "(74%): 18800000 lines in ../data/16-dec/parsed_retweets.json processed (0.33414721488952637 sec)\n",
      "(74%): 18900000 lines in ../data/16-dec/parsed_retweets.json processed (0.326840877532959 sec)\n",
      "(74%): 19000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3630058765411377 sec)\n",
      "(75%): 19100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3979320526123047 sec)\n",
      "(75%): 19200000 lines in ../data/16-dec/parsed_retweets.json processed (0.5693330764770508 sec)\n",
      "(75%): 19300000 lines in ../data/16-dec/parsed_retweets.json processed (0.41072988510131836 sec)\n",
      "(76%): 19400000 lines in ../data/16-dec/parsed_retweets.json processed (0.4102761745452881 sec)\n",
      "(76%): 19500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3905520439147949 sec)\n",
      "(77%): 19600000 lines in ../data/16-dec/parsed_retweets.json processed (0.5485448837280273 sec)\n",
      "(77%): 19700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3730762004852295 sec)\n",
      "(77%): 19800000 lines in ../data/16-dec/parsed_retweets.json processed (0.48106908798217773 sec)\n",
      "(78%): 19900000 lines in ../data/16-dec/parsed_retweets.json processed (0.38677501678466797 sec)\n",
      "(78%): 20000000 lines in ../data/16-dec/parsed_retweets.json processed (0.4219667911529541 sec)\n",
      "(79%): 20100000 lines in ../data/16-dec/parsed_retweets.json processed (0.3911609649658203 sec)\n",
      "(79%): 20200000 lines in ../data/16-dec/parsed_retweets.json processed (0.37590599060058594 sec)\n",
      "(79%): 20300000 lines in ../data/16-dec/parsed_retweets.json processed (0.37447500228881836 sec)\n",
      "(80%): 20400000 lines in ../data/16-dec/parsed_retweets.json processed (0.39989495277404785 sec)\n",
      "(80%): 20500000 lines in ../data/16-dec/parsed_retweets.json processed (0.41193389892578125 sec)\n",
      "(81%): 20600000 lines in ../data/16-dec/parsed_retweets.json processed (0.41262316703796387 sec)\n",
      "(81%): 20700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3631160259246826 sec)\n",
      "(81%): 20800000 lines in ../data/16-dec/parsed_retweets.json processed (0.37943220138549805 sec)\n",
      "(82%): 20900000 lines in ../data/16-dec/parsed_retweets.json processed (0.37791895866394043 sec)\n",
      "(82%): 21000000 lines in ../data/16-dec/parsed_retweets.json processed (0.36115407943725586 sec)\n",
      "(83%): 21100000 lines in ../data/16-dec/parsed_retweets.json processed (0.4341850280761719 sec)\n",
      "(83%): 21200000 lines in ../data/16-dec/parsed_retweets.json processed (0.3677990436553955 sec)\n",
      "(83%): 21300000 lines in ../data/16-dec/parsed_retweets.json processed (0.36601924896240234 sec)\n",
      "(84%): 21400000 lines in ../data/16-dec/parsed_retweets.json processed (0.39437317848205566 sec)\n",
      "(84%): 21500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3481438159942627 sec)\n",
      "(84%): 21600000 lines in ../data/16-dec/parsed_retweets.json processed (0.4155771732330322 sec)\n",
      "(85%): 21700000 lines in ../data/16-dec/parsed_retweets.json processed (0.37746500968933105 sec)\n",
      "(85%): 21800000 lines in ../data/16-dec/parsed_retweets.json processed (0.35694122314453125 sec)\n",
      "(86%): 21900000 lines in ../data/16-dec/parsed_retweets.json processed (0.3625202178955078 sec)\n",
      "(86%): 22000000 lines in ../data/16-dec/parsed_retweets.json processed (0.36296772956848145 sec)\n",
      "(86%): 22100000 lines in ../data/16-dec/parsed_retweets.json processed (0.34749698638916016 sec)\n",
      "(87%): 22200000 lines in ../data/16-dec/parsed_retweets.json processed (0.4239318370819092 sec)\n",
      "(87%): 22300000 lines in ../data/16-dec/parsed_retweets.json processed (0.3852989673614502 sec)\n",
      "(88%): 22400000 lines in ../data/16-dec/parsed_retweets.json processed (0.39588308334350586 sec)\n",
      "(88%): 22500000 lines in ../data/16-dec/parsed_retweets.json processed (0.4716973304748535 sec)\n",
      "(88%): 22600000 lines in ../data/16-dec/parsed_retweets.json processed (0.5404889583587646 sec)\n",
      "(89%): 22700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3720703125 sec)\n",
      "(89%): 22800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3555128574371338 sec)\n",
      "(90%): 22900000 lines in ../data/16-dec/parsed_retweets.json processed (0.34312009811401367 sec)\n",
      "(90%): 23000000 lines in ../data/16-dec/parsed_retweets.json processed (0.3827219009399414 sec)\n",
      "(90%): 23100000 lines in ../data/16-dec/parsed_retweets.json processed (0.4699668884277344 sec)\n",
      "(91%): 23200000 lines in ../data/16-dec/parsed_retweets.json processed (0.4353158473968506 sec)\n",
      "(91%): 23300000 lines in ../data/16-dec/parsed_retweets.json processed (0.37729787826538086 sec)\n",
      "(92%): 23400000 lines in ../data/16-dec/parsed_retweets.json processed (0.37951111793518066 sec)\n",
      "(92%): 23500000 lines in ../data/16-dec/parsed_retweets.json processed (0.4012300968170166 sec)\n",
      "(92%): 23600000 lines in ../data/16-dec/parsed_retweets.json processed (0.4222080707550049 sec)\n",
      "(93%): 23700000 lines in ../data/16-dec/parsed_retweets.json processed (0.4080371856689453 sec)\n",
      "(93%): 23800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3716261386871338 sec)\n",
      "(93%): 23900000 lines in ../data/16-dec/parsed_retweets.json processed (0.396651029586792 sec)\n",
      "(94%): 24000000 lines in ../data/16-dec/parsed_retweets.json processed (0.423583984375 sec)\n",
      "(94%): 24100000 lines in ../data/16-dec/parsed_retweets.json processed (0.5855979919433594 sec)\n",
      "(95%): 24200000 lines in ../data/16-dec/parsed_retweets.json processed (0.384613037109375 sec)\n",
      "(95%): 24300000 lines in ../data/16-dec/parsed_retweets.json processed (0.37354397773742676 sec)\n",
      "(95%): 24400000 lines in ../data/16-dec/parsed_retweets.json processed (0.42278194427490234 sec)\n",
      "(96%): 24500000 lines in ../data/16-dec/parsed_retweets.json processed (0.3856780529022217 sec)\n",
      "(96%): 24600000 lines in ../data/16-dec/parsed_retweets.json processed (0.4906151294708252 sec)\n",
      "(97%): 24700000 lines in ../data/16-dec/parsed_retweets.json processed (0.3742711544036865 sec)\n",
      "(97%): 24800000 lines in ../data/16-dec/parsed_retweets.json processed (0.3821878433227539 sec)\n",
      "(97%): 24900000 lines in ../data/16-dec/parsed_retweets.json processed (0.4707210063934326 sec)\n",
      "(98%): 25000000 lines in ../data/16-dec/parsed_retweets.json processed (0.5387759208679199 sec)\n",
      "(98%): 25100000 lines in ../data/16-dec/parsed_retweets.json processed (0.40019989013671875 sec)\n",
      "(99%): 25200000 lines in ../data/16-dec/parsed_retweets.json processed (0.40219807624816895 sec)\n",
      "(99%): 25300000 lines in ../data/16-dec/parsed_retweets.json processed (0.42270898818969727 sec)\n",
      "(99%): 25400000 lines in ../data/16-dec/parsed_retweets.json processed (0.4135549068450928 sec)\n",
      "(100%): 25500000 lines in ../data/16-dec/parsed_retweets.json processed (0.5467309951782227 sec)\n",
      "Done loading ../data/16-dec/parsed_retweets.json\n",
      "25566698 lines in ../data/16-dec/parsed_retweets.json processed (96.40407705307007 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25566698 entries, 0 to 25566697\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   user                object\n",
      " 1   timestamp           object\n",
      " 2   retweeted           object\n",
      " 3   retweetedFrom_user  object\n",
      "dtypes: object(4)\n",
      "memory usage: 780.2+ MB\n"
     ]
    }
   ],
   "source": [
    "retweet_df = create_retweet_df(data_dir=DATA_DIR)\n",
    "retweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2020-10-23T16:59:58Z'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "retweet_df.timestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 2696807 json lines\n",
      "(4%): 100000 lines in ../data/14-nov/parsed_tweets.json processed (2.0737709999084473 sec)\n",
      "(7%): 200000 lines in ../data/14-nov/parsed_tweets.json processed (2.269927978515625 sec)\n",
      "(11%): 300000 lines in ../data/14-nov/parsed_tweets.json processed (2.6733357906341553 sec)\n",
      "(15%): 400000 lines in ../data/14-nov/parsed_tweets.json processed (2.2250869274139404 sec)\n",
      "(19%): 500000 lines in ../data/14-nov/parsed_tweets.json processed (2.583611011505127 sec)\n",
      "(22%): 600000 lines in ../data/14-nov/parsed_tweets.json processed (2.8332784175872803 sec)\n",
      "(26%): 700000 lines in ../data/14-nov/parsed_tweets.json processed (1.5548789501190186 sec)\n",
      "(30%): 800000 lines in ../data/14-nov/parsed_tweets.json processed (3.0940160751342773 sec)\n",
      "(33%): 900000 lines in ../data/14-nov/parsed_tweets.json processed (1.6221098899841309 sec)\n",
      "(37%): 1000000 lines in ../data/14-nov/parsed_tweets.json processed (3.6446330547332764 sec)\n",
      "(41%): 1100000 lines in ../data/14-nov/parsed_tweets.json processed (1.5825090408325195 sec)\n",
      "(44%): 1200000 lines in ../data/14-nov/parsed_tweets.json processed (1.561845064163208 sec)\n",
      "(48%): 1300000 lines in ../data/14-nov/parsed_tweets.json processed (4.108792781829834 sec)\n",
      "(52%): 1400000 lines in ../data/14-nov/parsed_tweets.json processed (1.5221011638641357 sec)\n",
      "(56%): 1500000 lines in ../data/14-nov/parsed_tweets.json processed (1.4554917812347412 sec)\n",
      "(59%): 1600000 lines in ../data/14-nov/parsed_tweets.json processed (4.65038800239563 sec)\n",
      "(63%): 1700000 lines in ../data/14-nov/parsed_tweets.json processed (1.6858978271484375 sec)\n",
      "(67%): 1800000 lines in ../data/14-nov/parsed_tweets.json processed (1.762239933013916 sec)\n",
      "(70%): 1900000 lines in ../data/14-nov/parsed_tweets.json processed (1.6267468929290771 sec)\n",
      "(74%): 2000000 lines in ../data/14-nov/parsed_tweets.json processed (5.502563714981079 sec)\n",
      "(78%): 2100000 lines in ../data/14-nov/parsed_tweets.json processed (1.457205057144165 sec)\n",
      "(82%): 2200000 lines in ../data/14-nov/parsed_tweets.json processed (1.3944339752197266 sec)\n",
      "(85%): 2300000 lines in ../data/14-nov/parsed_tweets.json processed (1.3156509399414062 sec)\n",
      "(89%): 2400000 lines in ../data/14-nov/parsed_tweets.json processed (6.495137929916382 sec)\n",
      "(93%): 2500000 lines in ../data/14-nov/parsed_tweets.json processed (1.496802806854248 sec)\n",
      "(96%): 2600000 lines in ../data/14-nov/parsed_tweets.json processed (1.4589908123016357 sec)\n",
      "Done loading ../data/14-nov/parsed_tweets.json\n",
      "2696807 lines in ../data/14-nov/parsed_tweets.json processed (65.0266101360321 sec)\n"
     ]
    }
   ],
   "source": [
    "old_tweet_df, recent_tweet_df = create_tweet_df(\n",
    "    # retweet_df.timestamp.min()\n",
    "    '2020-10-23T16:59:58Z', \n",
    "    CRAWLED_TERMS, \n",
    "    data_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 1388621 json lines\n",
      "(7%): 100000 lines in ../data/16-dec/parsed_users.json processed (6.144344091415405 sec)\n",
      "(14%): 200000 lines in ../data/16-dec/parsed_users.json processed (4.8742687702178955 sec)\n",
      "(22%): 300000 lines in ../data/16-dec/parsed_users.json processed (4.657926797866821 sec)\n",
      "(29%): 400000 lines in ../data/16-dec/parsed_users.json processed (4.414881229400635 sec)\n",
      "(36%): 500000 lines in ../data/16-dec/parsed_users.json processed (4.135060787200928 sec)\n",
      "(43%): 600000 lines in ../data/16-dec/parsed_users.json processed (4.198259353637695 sec)\n",
      "(50%): 700000 lines in ../data/16-dec/parsed_users.json processed (4.581743001937866 sec)\n",
      "(58%): 800000 lines in ../data/16-dec/parsed_users.json processed (4.835165977478027 sec)\n",
      "(65%): 900000 lines in ../data/16-dec/parsed_users.json processed (5.120298862457275 sec)\n",
      "(72%): 1000000 lines in ../data/16-dec/parsed_users.json processed (4.460118055343628 sec)\n",
      "(79%): 1100000 lines in ../data/16-dec/parsed_users.json processed (4.700106143951416 sec)\n",
      "(86%): 1200000 lines in ../data/16-dec/parsed_users.json processed (5.502142906188965 sec)\n",
      "(94%): 1300000 lines in ../data/16-dec/parsed_users.json processed (5.108906984329224 sec)\n",
      "Done loading ../data/16-dec/parsed_users.json\n",
      "1388621 lines in ../data/16-dec/parsed_users.json processed (67.09096217155457 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1388621 entries, 1334881524664193027 to 3356902745\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count    Dtype \n",
      "---  ------           --------------    ----- \n",
      " 0   protected        1388621 non-null  bool  \n",
      " 1   friends          0 non-null        object\n",
      " 2   created_at       1388621 non-null  object\n",
      " 3   name             1388621 non-null  object\n",
      " 4   friends_count    1388621 non-null  int32 \n",
      " 5   verified         1388621 non-null  bool  \n",
      " 6   followers_count  1388621 non-null  int32 \n",
      " 7   location         809824 non-null   object\n",
      " 8   followed_cnts    1388621 non-null  int32 \n",
      " 9   handle           1388621 non-null  object\n",
      " 10  url              243012 non-null   object\n",
      "dtypes: bool(2), int32(3), object(6)\n",
      "memory usage: 92.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_users = create_user_df(data_dir=DATA_DIR)\n",
    "df_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 389799 json lines\n",
      "(26%): 100000 lines in ../data/16-dec//parsed_media.json processed (0.3682749271392822 sec)\n",
      "(51%): 200000 lines in ../data/16-dec//parsed_media.json processed (0.31970787048339844 sec)\n",
      "(77%): 300000 lines in ../data/16-dec//parsed_media.json processed (0.3132140636444092 sec)\n",
      "Done loading ../data/16-dec//parsed_media.json\n",
      "389799 lines in ../data/16-dec//parsed_media.json processed (1.2917020320892334 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 196517 entries, 1339030097194483712 to 1332824783738839040\n",
      "Data columns (total 49 columns):\n",
      " #   Column                      Non-Null Count   Dtype           \n",
      "---  ------                      --------------   -----           \n",
      " 0   datastore_id                196517 non-null  object          \n",
      " 1   type                        196517 non-null  object          \n",
      " 2   media_id                    196517 non-null  object          \n",
      " 3   media_url                   196517 non-null  object          \n",
      " 4   urls                        196280 non-null  object          \n",
      " 5   hasMedia                    196280 non-null  object          \n",
      " 6   hashtags                    196280 non-null  object          \n",
      " 7   retweet_count               196517 non-null  int32           \n",
      " 8   quote_count                 196517 non-null  int32           \n",
      " 9   user                        196280 non-null  object          \n",
      " 10  quote_tweet                 93873 non-null   object          \n",
      " 11  timestamp                   196280 non-null  object          \n",
      " 12  tokens                      196280 non-null  object          \n",
      " 13  election fraud              196517 non-null  Sparse[int64, 0]\n",
      " 14  voter fraud                 196517 non-null  Sparse[int64, 0]\n",
      " 15  #voterfraud                 196517 non-null  Sparse[int64, 0]\n",
      " 16  #stopthesteal               196517 non-null  Sparse[int64, 0]\n",
      " 17  #ballotharvesting           196517 non-null  Sparse[int64, 0]\n",
      " 18  ballot fraud                196517 non-null  Sparse[int64, 0]\n",
      " 19  #electionfraud              196517 non-null  Sparse[int64, 0]\n",
      " 20  #electioninterference       196517 non-null  Sparse[int64, 0]\n",
      " 21  ballot harvesting           196517 non-null  Sparse[int64, 0]\n",
      " 22  election interference       196517 non-null  Sparse[int64, 0]\n",
      " 23  #electiontampering          196517 non-null  Sparse[int64, 0]\n",
      " 24  #cheatingdemocrats          196517 non-null  Sparse[int64, 0]\n",
      " 25  election tampering          196517 non-null  Sparse[int64, 0]\n",
      " 26  democrats cheat             196517 non-null  Sparse[int64, 0]\n",
      " 27  #voterfraudisreal           196517 non-null  Sparse[int64, 0]\n",
      " 28  cheating democrats          196517 non-null  Sparse[int64, 0]\n",
      " 29  #gopvoterfraud              196517 non-null  Sparse[int64, 0]\n",
      " 30  destroyed ballots           196517 non-null  Sparse[int64, 0]\n",
      " 31  stolen ballots              196517 non-null  Sparse[int64, 0]\n",
      " 32  #ballotfraud                196517 non-null  Sparse[int64, 0]\n",
      " 33  discarded ballots           196517 non-null  Sparse[int64, 0]\n",
      " 34  hacked voting machine       196517 non-null  Sparse[int64, 0]\n",
      " 35  pre-filled ballot           196517 non-null  Sparse[int64, 0]\n",
      " 36  harvest ballot              196517 non-null  Sparse[int64, 0]\n",
      " 37  #stopvoterfraud             196517 non-null  Sparse[int64, 0]\n",
      " 38  #democratvoterfraud         196517 non-null  Sparse[int64, 0]\n",
      " 39  #ballotvoterfraud           196517 non-null  Sparse[int64, 0]\n",
      " 40  #nomailinvoting             196517 non-null  Sparse[int64, 0]\n",
      " 41  #ilhanomarballotharvesting  196517 non-null  Sparse[int64, 0]\n",
      " 42  vote by mail fraud          196517 non-null  Sparse[int64, 0]\n",
      " 43  #mailinvoterfraud           196517 non-null  Sparse[int64, 0]\n",
      " 44  #votebymailfraud            196517 non-null  Sparse[int64, 0]\n",
      " 45  #ilhanomarvoterfraud        196517 non-null  Sparse[int64, 0]\n",
      " 46  #stopgopvoterfraud          196517 non-null  Sparse[int64, 0]\n",
      " 47  #discardedballots           196517 non-null  Sparse[int64, 0]\n",
      " 48  #hackedvotingmachines       196517 non-null  Sparse[int64, 0]\n",
      "dtypes: Sparse[int64, 0](36), int32(2), object(11)\n",
      "memory usage: 29.7+ MB\n"
     ]
    }
   ],
   "source": [
    "from data_tools import lookup_parsed_data, load_parsed_data\n",
    "def create_media_df(tweet_df, data_dir=\"../data/14-nov/\"):\n",
    "    tweet_df_with_media = tweet_df[tweet_df['hasMedia'] == True].set_index('datastore_id')\n",
    "    # Preserve types when joining\n",
    "    col_types = tweet_df_with_media.select_dtypes(include=['int', 'int32']).dtypes\n",
    "    media_df = load_parsed_data(\n",
    "        data_dir + \"/parsed_media.json\",\n",
    "    ).drop_duplicates(\"media_id\").set_index(\"tweet_id\")\n",
    "    df_media_with_tweets = media_df.join(tweet_df_with_media, on='tweet_id')\n",
    "    for col, col_type in col_types.iteritems():\n",
    "        df_media_with_tweets[col] = df_media_with_tweets[col].fillna(0).astype(col_type)\n",
    "\n",
    "    return df_media_with_tweets\n",
    "\n",
    "df_media_with_tweets = create_media_df(recent_tweet_df, data_dir=DATA_DIR)\n",
    "df_media_with_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 36 entries, 35 to 15\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   term         36 non-null     object\n 1   tweet count  36 non-null     int64 \ndtypes: int64(1), object(1)\nmemory usage: 864.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "def create_crawled_terms_df(crawled_terms, tweet_df):\n",
    "    crawled_terms_stats = []\n",
    "\n",
    "    for term in crawled_terms:\n",
    "        if term in tweet_df.columns:\n",
    "            stats = {}\n",
    "            stats[\"term\"] = term\n",
    "            stats[\"tweet count\"] = tweet_df[term].value_counts().values[1]\n",
    "            crawled_terms_stats.append(stats)\n",
    "\n",
    "    crawled_terms_df = pd.DataFrame(crawled_terms_stats).sort_values(\n",
    "        by=[\"tweet count\"], ascending=False\n",
    "    )\n",
    "\n",
    "    return crawled_terms_df\n",
    "\n",
    "crawled_terms_df = create_crawled_terms_df(CRAWLED_TERMS, recent_tweet_df)\n",
    "crawled_terms_df.info()"
   ]
  },
  {
   "source": [
    "## Basic stats & Coverage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "coverage_stats = {}\n",
    "\n",
    "coverage_stats[\"old_tweet_count\"] = len(old_tweet_df.index)\n",
    "coverage_stats[\"recent_tweet_count\"] = len(recent_tweet_df.index)\n",
    "coverage_stats[\"total_tweet_count\"] = coverage_stats[\"recent_tweet_count\"] + coverage_stats[\"old_tweet_count\"]\n",
    "coverage_stats[\"retweet_count\"] = len(retweet_df.index)\n",
    "coverage_stats[\"user_count\"] = len(df_users.index)\n",
    "\n",
    "coverage_stats[\"earliest_tweet\"] = recent_tweet_df.timestamp.min()\n",
    "coverage_stats[\"latest_tweet\"] = recent_tweet_df.timestamp.max()\n",
    "coverage_stats[\"earliest_retweet\"] = retweet_df.timestamp.min()\n",
    "coverage_stats[\"latest_retweet\"] = retweet_df.timestamp.max()\n",
    "\n",
    "coverage_stats"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'old_tweet_count': 5902,\n",
       " 'recent_tweet_count': 7603103,\n",
       " 'total_tweet_count': 7609005,\n",
       " 'retweet_count': 25566698,\n",
       " 'user_count': 1388621,\n",
       " 'earliest_tweet': '2008-11-05T02:44:00Z',\n",
       " 'latest_tweet': '2020-12-16T13:08:49Z',\n",
       " 'earliest_retweet': '2020-10-23T16:59:58Z',\n",
       " 'latest_retweet': '2020-12-16T13:42:14Z'}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "source": [
    "## Terms grouped by hour"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           tweet count  retweet count  voter fraud  \\\n",
       "date                                                                 \n",
       "2020-10-23 17:00:00+00:00          306          681.0          179   \n",
       "2020-10-23 18:00:00+00:00          419         1272.0          238   \n",
       "2020-10-23 19:00:00+00:00          409          561.0          250   \n",
       "2020-10-23 20:00:00+00:00          645          932.0          372   \n",
       "2020-10-23 21:00:00+00:00          539          847.0          322   \n",
       "\n",
       "                           election fraud  #stopthesteal  #voterfraud  \\\n",
       "date                                                                    \n",
       "2020-10-23 17:00:00+00:00             4.0            0.0         24.0   \n",
       "2020-10-23 18:00:00+00:00             9.0            0.0         22.0   \n",
       "2020-10-23 19:00:00+00:00             8.0            1.0         50.0   \n",
       "2020-10-23 20:00:00+00:00            18.0            0.0         77.0   \n",
       "2020-10-23 21:00:00+00:00            11.0            0.0         57.0   \n",
       "\n",
       "                           #electionfraud  election interference  \\\n",
       "date                                                               \n",
       "2020-10-23 17:00:00+00:00             0.0                    0.0   \n",
       "2020-10-23 18:00:00+00:00             1.0                    0.0   \n",
       "2020-10-23 19:00:00+00:00             0.0                    2.0   \n",
       "2020-10-23 20:00:00+00:00             2.0                    2.0   \n",
       "2020-10-23 21:00:00+00:00             1.0                    0.0   \n",
       "\n",
       "                           ballot harvesting  ballot fraud  ...  \\\n",
       "date                                                        ...   \n",
       "2020-10-23 17:00:00+00:00                0.0           0.0  ...   \n",
       "2020-10-23 18:00:00+00:00                3.0           2.0  ...   \n",
       "2020-10-23 19:00:00+00:00                4.0           1.0  ...   \n",
       "2020-10-23 20:00:00+00:00                6.0           6.0  ...   \n",
       "2020-10-23 21:00:00+00:00                4.0           3.0  ...   \n",
       "\n",
       "                           hacked voting machine  pre-filled ballot  \\\n",
       "date                                                                  \n",
       "2020-10-23 17:00:00+00:00                    0.0                0.0   \n",
       "2020-10-23 18:00:00+00:00                    0.0                0.0   \n",
       "2020-10-23 19:00:00+00:00                    0.0                0.0   \n",
       "2020-10-23 20:00:00+00:00                    0.0                0.0   \n",
       "2020-10-23 21:00:00+00:00                    0.0                0.0   \n",
       "\n",
       "                           #ilhanomarballotharvesting  #ballotvoterfraud  \\\n",
       "date                                                                       \n",
       "2020-10-23 17:00:00+00:00                         0.0                0.0   \n",
       "2020-10-23 18:00:00+00:00                         0.0                0.0   \n",
       "2020-10-23 19:00:00+00:00                         0.0                0.0   \n",
       "2020-10-23 20:00:00+00:00                         0.0                0.0   \n",
       "2020-10-23 21:00:00+00:00                         0.0                0.0   \n",
       "\n",
       "                           #votebymailfraud  #nomailinvoting  \\\n",
       "date                                                           \n",
       "2020-10-23 17:00:00+00:00               0.0              0.0   \n",
       "2020-10-23 18:00:00+00:00               0.0              0.0   \n",
       "2020-10-23 19:00:00+00:00               0.0              0.0   \n",
       "2020-10-23 20:00:00+00:00               0.0              0.0   \n",
       "2020-10-23 21:00:00+00:00               0.0              0.0   \n",
       "\n",
       "                           #ilhanomarvoterfraud  #hackedvotingmachines  \\\n",
       "date                                                                     \n",
       "2020-10-23 17:00:00+00:00                   0.0                    0.0   \n",
       "2020-10-23 18:00:00+00:00                   0.0                    0.0   \n",
       "2020-10-23 19:00:00+00:00                   0.0                    0.0   \n",
       "2020-10-23 20:00:00+00:00                   0.0                    0.0   \n",
       "2020-10-23 21:00:00+00:00                   0.0                    0.0   \n",
       "\n",
       "                           #discardedballots  #stopgopvoterfraud  \n",
       "date                                                              \n",
       "2020-10-23 17:00:00+00:00                0.0                 0.0  \n",
       "2020-10-23 18:00:00+00:00                0.0                 0.0  \n",
       "2020-10-23 19:00:00+00:00                0.0                 0.0  \n",
       "2020-10-23 20:00:00+00:00                0.0                 0.0  \n",
       "2020-10-23 21:00:00+00:00                0.0                 0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet count</th>\n      <th>retweet count</th>\n      <th>voter fraud</th>\n      <th>election fraud</th>\n      <th>#stopthesteal</th>\n      <th>#voterfraud</th>\n      <th>#electionfraud</th>\n      <th>election interference</th>\n      <th>ballot harvesting</th>\n      <th>ballot fraud</th>\n      <th>...</th>\n      <th>hacked voting machine</th>\n      <th>pre-filled ballot</th>\n      <th>#ilhanomarballotharvesting</th>\n      <th>#ballotvoterfraud</th>\n      <th>#votebymailfraud</th>\n      <th>#nomailinvoting</th>\n      <th>#ilhanomarvoterfraud</th>\n      <th>#hackedvotingmachines</th>\n      <th>#discardedballots</th>\n      <th>#stopgopvoterfraud</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-10-23 17:00:00+00:00</th>\n      <td>306</td>\n      <td>681.0</td>\n      <td>179</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>24.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020-10-23 18:00:00+00:00</th>\n      <td>419</td>\n      <td>1272.0</td>\n      <td>238</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>22.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020-10-23 19:00:00+00:00</th>\n      <td>409</td>\n      <td>561.0</td>\n      <td>250</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>50.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020-10-23 20:00:00+00:00</th>\n      <td>645</td>\n      <td>932.0</td>\n      <td>372</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>77.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020-10-23 21:00:00+00:00</th>\n      <td>539</td>\n      <td>847.0</td>\n      <td>322</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  38 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_aggregated_by_hour = aggregate_counts_by_hour(recent_tweet_df, retweet_df, crawled_terms_df[\"term\"].values)\n",
    "df_aggregated_by_hour.head()"
   ]
  },
  {
   "source": [
    "## Most common hashtags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               all tweets  voter fraud  election fraud  #stopthesteal  \\\n",
       "hashtag                                                                 \n",
       "#1                      0          741             530              0   \n",
       "#12news                 0            0               0              0   \n",
       "#1a                     0            0               0              0   \n",
       "#2                      0            0               0              0   \n",
       "#2020election        9507          934             701           2282   \n",
       "\n",
       "               #voterfraud  #electionfraud  election interference  \\\n",
       "hashtag                                                             \n",
       "#1                       0               0                     47   \n",
       "#12news                  0               0                      0   \n",
       "#1a                      0               0                      0   \n",
       "#2                       0               0                      0   \n",
       "#2020election         2680            2608                     62   \n",
       "\n",
       "               ballot harvesting  ballot fraud  #electioninterference  ...  \\\n",
       "hashtag                                                                ...   \n",
       "#1                            21            18                      0  ...   \n",
       "#12news                        0             0                      0  ...   \n",
       "#1a                            0             0                      0  ...   \n",
       "#2                            12             0                      0  ...   \n",
       "#2020election                 14            19                    340  ...   \n",
       "\n",
       "               hacked voting machine  pre-filled ballot  \\\n",
       "hashtag                                                   \n",
       "#1                                 0                  0   \n",
       "#12news                            0                  0   \n",
       "#1a                                0                  0   \n",
       "#2                                 0                  0   \n",
       "#2020election                      0                  0   \n",
       "\n",
       "               #ilhanomarballotharvesting  #ballotvoterfraud  \\\n",
       "hashtag                                                        \n",
       "#1                                      0                  0   \n",
       "#12news                                 0                  0   \n",
       "#1a                                     0                  0   \n",
       "#2                                      0                  0   \n",
       "#2020election                           0                  2   \n",
       "\n",
       "               #votebymailfraud  #nomailinvoting  #ilhanomarvoterfraud  \\\n",
       "hashtag                                                                  \n",
       "#1                            0                0                     0   \n",
       "#12news                       0                0                     0   \n",
       "#1a                           0                0                     0   \n",
       "#2                            0                0                     0   \n",
       "#2020election                 0                0                     0   \n",
       "\n",
       "               #hackedvotingmachines  #discardedballots  #stopgopvoterfraud  \n",
       "hashtag                                                                      \n",
       "#1                                 0                  0                   0  \n",
       "#12news                            0                  0                   0  \n",
       "#1a                                0                  0                   0  \n",
       "#2                                 0                  0                   0  \n",
       "#2020election                      0                  0                   0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all tweets</th>\n      <th>voter fraud</th>\n      <th>election fraud</th>\n      <th>#stopthesteal</th>\n      <th>#voterfraud</th>\n      <th>#electionfraud</th>\n      <th>election interference</th>\n      <th>ballot harvesting</th>\n      <th>ballot fraud</th>\n      <th>#electioninterference</th>\n      <th>...</th>\n      <th>hacked voting machine</th>\n      <th>pre-filled ballot</th>\n      <th>#ilhanomarballotharvesting</th>\n      <th>#ballotvoterfraud</th>\n      <th>#votebymailfraud</th>\n      <th>#nomailinvoting</th>\n      <th>#ilhanomarvoterfraud</th>\n      <th>#hackedvotingmachines</th>\n      <th>#discardedballots</th>\n      <th>#stopgopvoterfraud</th>\n    </tr>\n    <tr>\n      <th>hashtag</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>#1</th>\n      <td>0</td>\n      <td>741</td>\n      <td>530</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>47</td>\n      <td>21</td>\n      <td>18</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>#12news</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>#1a</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>#2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>#2020election</th>\n      <td>9507</td>\n      <td>934</td>\n      <td>701</td>\n      <td>2282</td>\n      <td>2680</td>\n      <td>2608</td>\n      <td>62</td>\n      <td>14</td>\n      <td>19</td>\n      <td>340</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df_most_common_hashtags = aggregate_most_common_hashtags(recent_tweet_df, crawled_terms_df[\"term\"].values)\n",
    "df_most_common_hashtags.head()"
   ]
  },
  {
   "source": [
    "## Most common tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       all tweets  voter fraud  election fraud  #stopthesteal  #voterfraud  \\\n",
       "token                                                                        \n",
       "1               0            0               0              0            0   \n",
       "10              0            0               0              0            0   \n",
       "100             0            0               0              0            0   \n",
       "10000           0            0               0              0            0   \n",
       "1000s           0            0               0              0            0   \n",
       "\n",
       "       #electionfraud  election interference  ballot harvesting  ballot fraud  \\\n",
       "token                                                                           \n",
       "1                   0                      0                  0             0   \n",
       "10                  0                      0                  0             0   \n",
       "100                 0                      0                  0             0   \n",
       "10000               0                      0                  0             0   \n",
       "1000s               0                      0                  0             0   \n",
       "\n",
       "       #electioninterference  ...  hacked voting machine  pre-filled ballot  \\\n",
       "token                         ...                                             \n",
       "1                          0  ...                     17                  8   \n",
       "10                       206  ...                      0                  0   \n",
       "100                        0  ...                      0                  0   \n",
       "10000                      0  ...                      0                  0   \n",
       "1000s                      0  ...                      0                  0   \n",
       "\n",
       "       #ilhanomarballotharvesting  #ballotvoterfraud  #votebymailfraud  \\\n",
       "token                                                                    \n",
       "1                               0                  0                 1   \n",
       "10                              0                  0                 0   \n",
       "100                             0                  0                 0   \n",
       "10000                           0                  1                 0   \n",
       "1000s                           0                  0                 1   \n",
       "\n",
       "       #nomailinvoting  #ilhanomarvoterfraud  #hackedvotingmachines  \\\n",
       "token                                                                 \n",
       "1                    0                     0                      0   \n",
       "10                   0                     0                      0   \n",
       "100                  0                     0                      0   \n",
       "10000                0                     0                      0   \n",
       "1000s                0                     0                      0   \n",
       "\n",
       "       #discardedballots  #stopgopvoterfraud  \n",
       "token                                         \n",
       "1                      0                   0  \n",
       "10                     0                   0  \n",
       "100                    0                   0  \n",
       "10000                  0                   0  \n",
       "1000s                  0                   0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all tweets</th>\n      <th>voter fraud</th>\n      <th>election fraud</th>\n      <th>#stopthesteal</th>\n      <th>#voterfraud</th>\n      <th>#electionfraud</th>\n      <th>election interference</th>\n      <th>ballot harvesting</th>\n      <th>ballot fraud</th>\n      <th>#electioninterference</th>\n      <th>...</th>\n      <th>hacked voting machine</th>\n      <th>pre-filled ballot</th>\n      <th>#ilhanomarballotharvesting</th>\n      <th>#ballotvoterfraud</th>\n      <th>#votebymailfraud</th>\n      <th>#nomailinvoting</th>\n      <th>#ilhanomarvoterfraud</th>\n      <th>#hackedvotingmachines</th>\n      <th>#discardedballots</th>\n      <th>#stopgopvoterfraud</th>\n    </tr>\n    <tr>\n      <th>token</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>17</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>206</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10000</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1000s</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS = STOP_WORDS.union({\"pron\", \"\", \" \", \"\", \"\", \"\"})\n",
    "\n",
    "def include_token(token):\n",
    "    return token not in STOP_WORDS and not token.startswith(\"hashtag\")\n",
    "\n",
    "def create_most_common_tokens_df(df_tweets, count_label, k=100):\n",
    "    counted_tokens = Counter(\n",
    "        [\n",
    "            token\n",
    "            for tokens in df_tweets[\"tokens\"]\n",
    "            for token in tokens\n",
    "            if include_token(token)\n",
    "        ]\n",
    "    )\n",
    "    return pd.DataFrame(\n",
    "        counted_tokens.most_common(k), columns=[\"token\", count_label]\n",
    "    ).set_index(\"token\")\n",
    "\n",
    "def aggregate_most_common_tokens(df_tweets, crawled_terms, k=100):\n",
    "    df_most_common_tokens = create_most_common_tokens_df(df_tweets, count_label=\"all tweets\", k=k)\n",
    "    for term in crawled_terms:\n",
    "        filtered_by_crawled_term = df_tweets[\n",
    "            df_tweets[term] == 1\n",
    "        ]\n",
    "        df_most_common_tokens = df_most_common_tokens.join(\n",
    "            create_most_common_tokens_df(filtered_by_crawled_term, count_label=term, k=k),\n",
    "            how='outer'\n",
    "        )\n",
    "    return df_most_common_tokens.fillna(0).astype(int)\n",
    "\n",
    "\n",
    "df_most_common_tokens = aggregate_most_common_tokens(recent_tweet_df, crawled_terms_df[\"term\"].values)\n",
    "df_most_common_tokens.head()"
   ]
  },
  {
   "source": [
    "## Co-occurrence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_term_threshold = 5000\n",
    "filtered_crawled_terms = crawled_terms_df[\n",
    "    crawled_terms_df[\"tweet count\"] > crawled_term_threshold\n",
    "]\n",
    "terms_in_df = [term for term in filtered_crawled_terms[\"term\"]]\n",
    "crawled_terms_tweet_df = (\n",
    "    recent_tweet_df[terms_in_df].sparse.to_dense().astype(\"int32\")\n",
    ")\n",
    "df_cooccurrence = crawled_terms_tweet_df.T.dot(crawled_terms_tweet_df)"
   ]
  },
  {
   "source": [
    "## Quote Tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quote_tweets = recent_tweet_df[recent_tweet_df[\"quote_tweet\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quote tweet count to coverage stats and update earliest_tweet\n",
    "with open(EXPORT_DIR + \"coverage_stats.pickle\", \"rb\") as f:\n",
    "    coverage_stats = pickle.load(f)\n",
    "    coverage_stats[\"quote_tweet_count\"] = df_quote_tweets.shape[0]\n",
    "    coverage_stats[\"earliest_tweet\"] = recent_tweet_df[\"timestamp\"].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'old_tweet_count': 5902, 'recent_tweet_count': 7603103, 'total_tweet_count': 7609005, 'retweet_count': 25566698, 'user_count': 1388621, 'earliest_tweet': '2020-10-23T17:00:04Z', 'latest_tweet': '2020-12-16T13:08:49Z', 'earliest_retweet': '2020-10-23T16:59:58Z', 'latest_retweet': '2020-12-16T13:42:14Z', 'quote_tweet_count': 3821579}\n"
     ]
    }
   ],
   "source": [
    "print(coverage_stats)"
   ]
  },
  {
   "source": [
    "## Top tweets by week\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                               datastore_id  quote_count  retweet_count  \\\n",
       "WeekDate     user                                                         \n",
       "2020-10-19 0 259001548  1320117044612943872        10199          34530   \n",
       "           1 341194704  1320107370312323073         9158          22217   \n",
       "           2 25073877   1320168905856397326          941          17118   \n",
       "           3 137752344  1320161049769889792         2846          15897   \n",
       "           4 39349894   1320160474672095232         3736          14948   \n",
       "\n",
       "                       hasMedia  \\\n",
       "WeekDate     user                 \n",
       "2020-10-19 0 259001548    False   \n",
       "           1 341194704    False   \n",
       "           2 25073877     False   \n",
       "           3 137752344    False   \n",
       "           4 39349894     False   \n",
       "\n",
       "                                                                     text  \\\n",
       "WeekDate     user                                                           \n",
       "2020-10-19 0 259001548   BIDEN ADMITS TO VOTER FRAUD! \\n\\n@JoeBide...   \n",
       "           1 341194704   \\n\\nJoe Biden brags about having the most ...   \n",
       "           2 25073877   Law Enforcement is watching and involved. So d...   \n",
       "           3 137752344  I hope youve been hacked, @GovMikeHuckabee. T...   \n",
       "           4 39349894   \\n\\nBiden: We have put together the most e...   \n",
       "\n",
       "                                   timestamp           handle  \\\n",
       "WeekDate     user                                               \n",
       "2020-10-19 0 259001548  2020-10-24T21:36:44Z  kayleighmcenany   \n",
       "           1 341194704  2020-10-24T20:58:18Z       SteveGuest   \n",
       "           2 25073877   2020-10-25T01:02:49Z  realDonaldTrump   \n",
       "           3 137752344  2020-10-25T00:31:36Z  EllenLWeintraub   \n",
       "           4 39349894   2020-10-25T00:29:19Z        EricTrump   \n",
       "\n",
       "                                       name  \n",
       "WeekDate     user                            \n",
       "2020-10-19 0 259001548     Kayleigh McEnany  \n",
       "           1 341194704          Steve Guest  \n",
       "           2 25073877       Donald J. Trump  \n",
       "           3 137752344  Ellen L  Weintraub  \n",
       "           4 39349894            Eric Trump  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>datastore_id</th>\n      <th>quote_count</th>\n      <th>retweet_count</th>\n      <th>hasMedia</th>\n      <th>text</th>\n      <th>timestamp</th>\n      <th>handle</th>\n      <th>name</th>\n    </tr>\n    <tr>\n      <th>WeekDate</th>\n      <th></th>\n      <th>user</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2020-10-19</th>\n      <th>0</th>\n      <th>259001548</th>\n      <td>1320117044612943872</td>\n      <td>10199</td>\n      <td>34530</td>\n      <td>False</td>\n      <td> BIDEN ADMITS TO VOTER FRAUD! \\n\\n@JoeBide...</td>\n      <td>2020-10-24T21:36:44Z</td>\n      <td>kayleighmcenany</td>\n      <td>Kayleigh McEnany</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>341194704</th>\n      <td>1320107370312323073</td>\n      <td>9158</td>\n      <td>22217</td>\n      <td>False</td>\n      <td> \\n\\nJoe Biden brags about having the most ...</td>\n      <td>2020-10-24T20:58:18Z</td>\n      <td>SteveGuest</td>\n      <td>Steve Guest</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>25073877</th>\n      <td>1320168905856397326</td>\n      <td>941</td>\n      <td>17118</td>\n      <td>False</td>\n      <td>Law Enforcement is watching and involved. So d...</td>\n      <td>2020-10-25T01:02:49Z</td>\n      <td>realDonaldTrump</td>\n      <td>Donald J. Trump</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <th>137752344</th>\n      <td>1320161049769889792</td>\n      <td>2846</td>\n      <td>15897</td>\n      <td>False</td>\n      <td>I hope youve been hacked, @GovMikeHuckabee. T...</td>\n      <td>2020-10-25T00:31:36Z</td>\n      <td>EllenLWeintraub</td>\n      <td>Ellen L  Weintraub</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <th>39349894</th>\n      <td>1320160474672095232</td>\n      <td>3736</td>\n      <td>14948</td>\n      <td>False</td>\n      <td>\\n\\nBiden: We have put together the most e...</td>\n      <td>2020-10-25T00:29:19Z</td>\n      <td>EricTrump</td>\n      <td>Eric Trump</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import datetime as dt\n",
    "tweets = recent_tweet_df\n",
    "tweets['WeekDate'] = pd.to_datetime(tweets.timestamp).dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weeks = tweets['WeekDate'].unique()\n",
    "\n",
    "def extract_weekly_tweets(df_week, N=25):\n",
    "    media_tweets = df_week[df_week[\"hasMedia\"]]\n",
    "    top_retweeted = df_week.nlargest(N, \"retweet_count\")\n",
    "    top_quoted = df_week.nlargest(N, \"quote_count\")\n",
    "    media_top_retweeted = media_tweets.nlargest(N, \"retweet_count\")\n",
    "    media_top_quoted = media_tweets.nlargest(N, \"quote_count\")\n",
    "    return (top_retweeted.merge(top_quoted, how=\"outer\")\n",
    "                        .merge(media_top_retweeted, how=\"outer\")\n",
    "                        .merge(media_top_quoted, how=\"outer\"))\n",
    "\n",
    "columns = [\"datastore_id\", \"quote_count\", \"retweet_count\", \"hasMedia\", \"text\", \"user\", \"timestamp\"]\n",
    "weekly_top_tweets = (tweets\n",
    "    .groupby(\"WeekDate\")[columns]\n",
    "    .apply(lambda df: extract_weekly_tweets(df))\n",
    "#  Merge with user df and sort by week date, retweet count\n",
    "    .merge(df_users[[\"handle\", \"name\"]], how='inner', left_on=\"user\", right_index=True)\n",
    "    .set_index(\"user\", append=True)\n",
    "    .sort_values(by=['WeekDate', 'retweet_count'], ascending=[True, False]))\n",
    "weekly_top_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(554, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "weekly_top_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(weeks))"
   ]
  },
  {
   "source": [
    "## Top users by week\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                retweeted_count  quoted_count  \\\n",
       "WeekDate   user                                                 \n",
       "2020-10-19 1316429019198500865               11             3   \n",
       "           1225026641614774273                0             0   \n",
       "           1225835542765395968                0             0   \n",
       "           2937103496                         0             0   \n",
       "           410022823                          0             0   \n",
       "\n",
       "                                all_tweet_count  media_tweet_count  \\\n",
       "WeekDate   user                                                      \n",
       "2020-10-19 1316429019198500865               33                 29   \n",
       "           1225026641614774273               12                 11   \n",
       "           1225835542765395968               10                 10   \n",
       "           2937103496                         9                  8   \n",
       "           410022823                         13                  8   \n",
       "\n",
       "                                quote_tweet_count  protected friends  \\\n",
       "WeekDate   user                                                        \n",
       "2020-10-19 1316429019198500865                  1      False    None   \n",
       "           1225026641614774273                  0      False    None   \n",
       "           1225835542765395968                  0      False    None   \n",
       "           2937103496                           0      False    None   \n",
       "           410022823                           13      False    None   \n",
       "\n",
       "                                          created_at                     name  \\\n",
       "WeekDate   user                                                                 \n",
       "2020-10-19 1316429019198500865  2020-10-14T17:22:27Z          Chalmatian70043   \n",
       "           1225026641614774273  2020-02-05T12:01:42Z        Philip Castilleja   \n",
       "           1225835542765395968  2020-02-07T17:35:59Z            FedupAmerican   \n",
       "           2937103496           2014-12-20T13:06:56Z  Gee.Frank is back y'all   \n",
       "           410022823            2011-11-11T14:49:06Z                    Alan7   \n",
       "\n",
       "                                friends_count  verified  followers_count  \\\n",
       "WeekDate   user                                                            \n",
       "2020-10-19 1316429019198500865             56     False                8   \n",
       "           1225026641614774273             68     False               31   \n",
       "           1225835542765395968           4971     False             4477   \n",
       "           2937103496                    1927     False             2592   \n",
       "           410022823                     4982     False             3349   \n",
       "\n",
       "                                   location  followed_cnts           handle  \\\n",
       "WeekDate   user                                                               \n",
       "2020-10-19 1316429019198500865         None              0  chalmatian70043   \n",
       "           1225026641614774273         None              0  PhilipCastille2   \n",
       "           1225835542765395968         None              0  FedupAm48219051   \n",
       "           2937103496           Alaska, USA              0    Frank_N_Meems   \n",
       "           410022823                   None              0         gohavfun   \n",
       "\n",
       "                                 url  \n",
       "WeekDate   user                       \n",
       "2020-10-19 1316429019198500865  None  \n",
       "           1225026641614774273  None  \n",
       "           1225835542765395968  None  \n",
       "           2937103496           None  \n",
       "           410022823            None  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>retweeted_count</th>\n      <th>quoted_count</th>\n      <th>all_tweet_count</th>\n      <th>media_tweet_count</th>\n      <th>quote_tweet_count</th>\n      <th>protected</th>\n      <th>friends</th>\n      <th>created_at</th>\n      <th>name</th>\n      <th>friends_count</th>\n      <th>verified</th>\n      <th>followers_count</th>\n      <th>location</th>\n      <th>followed_cnts</th>\n      <th>handle</th>\n      <th>url</th>\n    </tr>\n    <tr>\n      <th>WeekDate</th>\n      <th>user</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2020-10-19</th>\n      <th>1316429019198500865</th>\n      <td>11</td>\n      <td>3</td>\n      <td>33</td>\n      <td>29</td>\n      <td>1</td>\n      <td>False</td>\n      <td>None</td>\n      <td>2020-10-14T17:22:27Z</td>\n      <td>Chalmatian70043</td>\n      <td>56</td>\n      <td>False</td>\n      <td>8</td>\n      <td>None</td>\n      <td>0</td>\n      <td>chalmatian70043</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1225026641614774273</th>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>11</td>\n      <td>0</td>\n      <td>False</td>\n      <td>None</td>\n      <td>2020-02-05T12:01:42Z</td>\n      <td>Philip Castilleja</td>\n      <td>68</td>\n      <td>False</td>\n      <td>31</td>\n      <td>None</td>\n      <td>0</td>\n      <td>PhilipCastille2</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1225835542765395968</th>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>10</td>\n      <td>0</td>\n      <td>False</td>\n      <td>None</td>\n      <td>2020-02-07T17:35:59Z</td>\n      <td>FedupAmerican</td>\n      <td>4971</td>\n      <td>False</td>\n      <td>4477</td>\n      <td>None</td>\n      <td>0</td>\n      <td>FedupAm48219051</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2937103496</th>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>8</td>\n      <td>0</td>\n      <td>False</td>\n      <td>None</td>\n      <td>2014-12-20T13:06:56Z</td>\n      <td>Gee.Frank is back y'all</td>\n      <td>1927</td>\n      <td>False</td>\n      <td>2592</td>\n      <td>Alaska, USA</td>\n      <td>0</td>\n      <td>Frank_N_Meems</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>410022823</th>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>8</td>\n      <td>13</td>\n      <td>False</td>\n      <td>None</td>\n      <td>2011-11-11T14:49:06Z</td>\n      <td>Alan7</td>\n      <td>4982</td>\n      <td>False</td>\n      <td>3349</td>\n      <td>None</td>\n      <td>0</td>\n      <td>gohavfun</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import datetime as dt\n",
    "tweets = recent_tweet_df\n",
    "tweets['WeekDate'] = pd.to_datetime(tweets.timestamp).dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weeks = tweets['WeekDate'].unique()\n",
    "\n",
    "def extract_top_users(df_week, N=25):\n",
    "    top_retweeted = df_week.nlargest(N, \"retweeted_count\").reset_index(level=[\"user\"])\n",
    "    top_quoted = df_week.nlargest(N, \"quoted_count\").reset_index(level=[\"user\"])\n",
    "    most_tweets = df_week.nlargest(N, \"all_tweet_count\").reset_index(level=[\"user\"])\n",
    "    most_media_tweets = df_week.nlargest(N, \"media_tweet_count\").reset_index(level=[\"user\"])\n",
    "    most_quote_tweets = df_week.nlargest(N, \"quote_tweet_count\").reset_index(level=[\"user\"])\n",
    "    return (top_retweeted.merge(top_quoted, how=\"outer\")\n",
    "                         .merge(most_tweets, how=\"outer\")\n",
    "                         .merge(most_media_tweets, how=\"outer\")\n",
    "                         .merge(most_quote_tweets, how=\"outer\")).set_index(\"user\")\n",
    "\n",
    "columns = tweets.columns\n",
    "weekly_top_users = (tweets.groupby([\"WeekDate\", \"user\"])\n",
    "    .agg(\n",
    "        retweeted_count=('retweet_count', 'sum'), \n",
    "        quoted_count=('quote_count', 'sum'), \n",
    "        all_tweet_count=('datastore_id', 'count'),\n",
    "        media_tweet_count=('hasMedia', 'sum'),\n",
    "        quote_tweet_count=('quote_tweet', 'count')\n",
    "    )\n",
    "    .groupby(level=0, axis=0)\n",
    "    .apply(lambda df: extract_top_users(df))\n",
    "    #  Merge with user df and sort by week date, retweet count\n",
    "    .reset_index(level=[\"user\"])\n",
    "    .merge(df_users, how='inner', left_on=\"user\", right_index=True)\n",
    "    .set_index(\"user\", append=True)\n",
    "    .sort_values(by=['WeekDate', 'media_tweet_count'], ascending=[True, False]))\n",
    "\n",
    "weekly_top_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(804, 16)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "weekly_top_users.shape"
   ]
  },
  {
   "source": [
    "## Export"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df_users.to_pickle(EXPORT_DIR + \"df_users.pickle\")\n",
    "retweet_df.to_pickle(EXPORT_DIR + \"df_retweets.pickle\")\n",
    "recent_tweet_df.to_pickle(EXPORT_DIR + \"df_recent_tweets.pickle\")\n",
    "old_tweet_df.to_pickle(EXPORT_DIR + \"df_old_tweets.pickle\")\n",
    "df_aggregated_by_hour.to_pickle(EXPORT_DIR + \"df_counts_by_hour.pickle\")\n",
    "crawled_terms_df.to_pickle(EXPORT_DIR + \"df_crawled_terms.pickle\")\n",
    "df_most_common_hashtags.to_pickle(EXPORT_DIR + \"df_most_common_hashtags.pickle\")\n",
    "df_most_common_tokens.to_pickle(EXPORT_DIR + \"df_most_common_tokens.pickle\")\n",
    "df_cooccurrence.to_pickle(EXPORT_DIR + \"df_cooccurrence.pickle\")\n",
    "df_media_with_tweets.to_pickle(EXPORT_DIR + \"df_media_with_tweets.pickle\")\n",
    "df_quote_tweets.to_pickle(EXPORT_DIR + \"df_quote_tweets.pickle\")\n",
    "weekly_top_tweets.to_pickle(EXPORT_DIR + \"df_weekly_top_tweets.pickle\")\n",
    "weekly_top_users.to_pickle(EXPORT_DIR + \"df_weekly_top_users.pickle\")\n",
    "\n",
    "with open(EXPORT_DIR + \"coverage_stats.pickle\", \"wb\") as f:\n",
    "    pickle.dump(coverage_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quote_tweets.to_pickle(EXPORT_DIR + \"df_quote_tweets.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_DIR + \"coverage_stats.pickle\", \"wb\") as f:\n",
    "    pickle.dump(coverage_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_top_tweets.to_pickle(EXPORT_DIR + \"df_weekly_top_tweets.pickle\")\n",
    "weekly_top_users.to_pickle(EXPORT_DIR + \"df_weekly_top_users.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tweet_df.to_pickle(EXPORT_DIR + \"df_recent_tweets.pickle\")\n",
    "old_tweet_df.to_pickle(EXPORT_DIR + \"df_old_tweets.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_top_tweets.to_pickle(EXPORT_DIR + \"df_weekly_top_tweets.pickle\")"
   ]
  }
 ]
}