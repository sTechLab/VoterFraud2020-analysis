{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('voter-fraud': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a0f90dcbb54bea4e60f894f8fd1d686cc0f74395b4029405cc9f13e0b975e641"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import setup, load_tweet_df, load_media_df\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 2696807 json lines\n",
      "(4%): 100000 lines in ../data/14-nov/parsed_tweets.json processed (2.5799708366394043 sec)\n",
      "(7%): 200000 lines in ../data/14-nov/parsed_tweets.json processed (3.266613006591797 sec)\n",
      "(11%): 300000 lines in ../data/14-nov/parsed_tweets.json processed (2.7051432132720947 sec)\n",
      "(15%): 400000 lines in ../data/14-nov/parsed_tweets.json processed (3.0368881225585938 sec)\n",
      "(19%): 500000 lines in ../data/14-nov/parsed_tweets.json processed (3.1282520294189453 sec)\n",
      "(22%): 600000 lines in ../data/14-nov/parsed_tweets.json processed (3.4758729934692383 sec)\n",
      "(26%): 700000 lines in ../data/14-nov/parsed_tweets.json processed (3.7959330081939697 sec)\n",
      "(30%): 800000 lines in ../data/14-nov/parsed_tweets.json processed (2.4272072315216064 sec)\n",
      "(33%): 900000 lines in ../data/14-nov/parsed_tweets.json processed (4.138207912445068 sec)\n",
      "(37%): 1000000 lines in ../data/14-nov/parsed_tweets.json processed (2.437023878097534 sec)\n",
      "(41%): 1100000 lines in ../data/14-nov/parsed_tweets.json processed (4.717300891876221 sec)\n",
      "(44%): 1200000 lines in ../data/14-nov/parsed_tweets.json processed (2.0538270473480225 sec)\n",
      "(48%): 1300000 lines in ../data/14-nov/parsed_tweets.json processed (2.1941370964050293 sec)\n",
      "(52%): 1400000 lines in ../data/14-nov/parsed_tweets.json processed (5.091881036758423 sec)\n",
      "(56%): 1500000 lines in ../data/14-nov/parsed_tweets.json processed (2.2713358402252197 sec)\n",
      "(59%): 1600000 lines in ../data/14-nov/parsed_tweets.json processed (2.082596778869629 sec)\n",
      "(63%): 1700000 lines in ../data/14-nov/parsed_tweets.json processed (6.113768815994263 sec)\n",
      "(67%): 1800000 lines in ../data/14-nov/parsed_tweets.json processed (2.0651321411132812 sec)\n",
      "(70%): 1900000 lines in ../data/14-nov/parsed_tweets.json processed (2.168865919113159 sec)\n",
      "(74%): 2000000 lines in ../data/14-nov/parsed_tweets.json processed (2.183568239212036 sec)\n",
      "(78%): 2100000 lines in ../data/14-nov/parsed_tweets.json processed (6.826121091842651 sec)\n",
      "(82%): 2200000 lines in ../data/14-nov/parsed_tweets.json processed (2.711019992828369 sec)\n",
      "(85%): 2300000 lines in ../data/14-nov/parsed_tweets.json processed (2.1025872230529785 sec)\n",
      "(89%): 2400000 lines in ../data/14-nov/parsed_tweets.json processed (2.221458911895752 sec)\n",
      "(93%): 2500000 lines in ../data/14-nov/parsed_tweets.json processed (2.0885303020477295 sec)\n",
      "(96%): 2600000 lines in ../data/14-nov/parsed_tweets.json processed (2.4100308418273926 sec)\n",
      "Done loading ../data/14-nov/parsed_tweets.json\n",
      "2696807 lines in ../data/14-nov/parsed_tweets.json processed (89.43797516822815 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2696807 entries, 0 to 2696806\n",
      "Data columns (total 45 columns):\n",
      " #   Column                      Dtype          \n",
      "---  ------                      -----          \n",
      " 0   datastore_id                object         \n",
      " 1   hashtags                    object         \n",
      " 2   urls                        object         \n",
      " 3   hasMedia                    bool           \n",
      " 4   quote_tweet                 object         \n",
      " 5   retweet_count               int32          \n",
      " 6   timestamp                   object         \n",
      " 7   quote_count                 int32          \n",
      " 8   user                        object         \n",
      " 9   tokens                      object         \n",
      " 10  voter fraud                 Sparse[int8, 0]\n",
      " 11  #electionfraud              Sparse[int8, 0]\n",
      " 12  ballot fraud                Sparse[int8, 0]\n",
      " 13  #voterfraud                 Sparse[int8, 0]\n",
      " 14  #stopthesteal               Sparse[int8, 0]\n",
      " 15  stolen ballots              Sparse[int8, 0]\n",
      " 16  election fraud              Sparse[int8, 0]\n",
      " 17  ballot harvesting           Sparse[int8, 0]\n",
      " 18  #electioninterference       Sparse[int8, 0]\n",
      " 19  #cheatingdemocrats          Sparse[int8, 0]\n",
      " 20  election interference       Sparse[int8, 0]\n",
      " 21  #ballotfraud                Sparse[int8, 0]\n",
      " 22  cheating democrats          Sparse[int8, 0]\n",
      " 23  #electiontampering          Sparse[int8, 0]\n",
      " 24  #ballotharvesting           Sparse[int8, 0]\n",
      " 25  #voterfraudisreal           Sparse[int8, 0]\n",
      " 26  destroyed ballots           Sparse[int8, 0]\n",
      " 27  democrats cheat             Sparse[int8, 0]\n",
      " 28  #stopvoterfraud             Sparse[int8, 0]\n",
      " 29  #ballotvoterfraud           Sparse[int8, 0]\n",
      " 30  election tampering          Sparse[int8, 0]\n",
      " 31  #democratvoterfraud         Sparse[int8, 0]\n",
      " 32  discarded ballots           Sparse[int8, 0]\n",
      " 33  vote by mail fraud          Sparse[int8, 0]\n",
      " 34  harvest ballot              Sparse[int8, 0]\n",
      " 35  #gopvoterfraud              Sparse[int8, 0]\n",
      " 36  #nomailinvoting             Sparse[int8, 0]\n",
      " 37  #votebymailfraud            Sparse[int8, 0]\n",
      " 38  #mailinvoterfraud           Sparse[int8, 0]\n",
      " 39  pre-filled ballot           Sparse[int8, 0]\n",
      " 40  hacked voting machine       Sparse[int8, 0]\n",
      " 41  #ilhanomarballotharvesting  Sparse[int8, 0]\n",
      " 42  #ilhanomarvoterfraud        Sparse[int8, 0]\n",
      " 43  #hackedvotingmachines       Sparse[int8, 0]\n",
      " 44  #discardedballots           Sparse[int8, 0]\n",
      "dtypes: Sparse[int8, 0](35), bool(1), int32(2), object(7)\n",
      "memory usage: 173.9+ MB\n"
     ]
    }
   ],
   "source": [
    "from data_tools import load_crawled_terms, load_parsed_data\n",
    "\n",
    "crawled_terms, crawled_hashtags, crawled_phrases = load_crawled_terms(\"../keywords-3nov.txt\", split_hashtags=True)\n",
    "\n",
    "cast_cols = {\n",
    "    \"tweet_count\": \"int32\",\n",
    "    \"quote_count\": \"int32\" \n",
    "}\n",
    "for term in crawled_terms:\n",
    "    cast_cols[term] = \"Sparse[int8]\"\n",
    "\n",
    "tweet_df = load_parsed_data('../data/14-nov/parsed_tweets.json', exclude_cols={\n",
    "    \"cleaned_text\", \n",
    "    \"entities\",\n",
    "    \"replyTo\",\n",
    "    \"replyTo_user\",\n",
    "    \"text\", \n",
    "    \"last_retweeted\", \n",
    "    \"place\", \n",
    "    \"processed\",\n",
    "    \"media\", \n",
    "    \"isDeleted\"\n",
    "}, cast_cols=cast_cols, verbose=True)\n",
    "\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 8044982 json lines\n",
      "(1%): 100000 lines in ../data/14-nov/parsed_retweets.json processed (0.41941189765930176 sec)\n",
      "(2%): 200000 lines in ../data/14-nov/parsed_retweets.json processed (0.6255569458007812 sec)\n",
      "(4%): 300000 lines in ../data/14-nov/parsed_retweets.json processed (0.43156933784484863 sec)\n",
      "(5%): 400000 lines in ../data/14-nov/parsed_retweets.json processed (0.4770238399505615 sec)\n",
      "(6%): 500000 lines in ../data/14-nov/parsed_retweets.json processed (0.4291038513183594 sec)\n",
      "(7%): 600000 lines in ../data/14-nov/parsed_retweets.json processed (0.4238567352294922 sec)\n",
      "(9%): 700000 lines in ../data/14-nov/parsed_retweets.json processed (0.3710439205169678 sec)\n",
      "(10%): 800000 lines in ../data/14-nov/parsed_retweets.json processed (0.37425804138183594 sec)\n",
      "(11%): 900000 lines in ../data/14-nov/parsed_retweets.json processed (0.46216392517089844 sec)\n",
      "(12%): 1000000 lines in ../data/14-nov/parsed_retweets.json processed (0.4916369915008545 sec)\n",
      "(14%): 1100000 lines in ../data/14-nov/parsed_retweets.json processed (0.6163408756256104 sec)\n",
      "(15%): 1200000 lines in ../data/14-nov/parsed_retweets.json processed (0.4836747646331787 sec)\n",
      "(16%): 1300000 lines in ../data/14-nov/parsed_retweets.json processed (0.611616849899292 sec)\n",
      "(17%): 1400000 lines in ../data/14-nov/parsed_retweets.json processed (0.4078078269958496 sec)\n",
      "(19%): 1500000 lines in ../data/14-nov/parsed_retweets.json processed (0.5960431098937988 sec)\n",
      "(20%): 1600000 lines in ../data/14-nov/parsed_retweets.json processed (0.3979029655456543 sec)\n",
      "(21%): 1700000 lines in ../data/14-nov/parsed_retweets.json processed (0.47042298316955566 sec)\n",
      "(22%): 1800000 lines in ../data/14-nov/parsed_retweets.json processed (0.39329004287719727 sec)\n",
      "(24%): 1900000 lines in ../data/14-nov/parsed_retweets.json processed (0.42853403091430664 sec)\n",
      "(25%): 2000000 lines in ../data/14-nov/parsed_retweets.json processed (0.4384040832519531 sec)\n",
      "(26%): 2100000 lines in ../data/14-nov/parsed_retweets.json processed (0.5313138961791992 sec)\n",
      "(27%): 2200000 lines in ../data/14-nov/parsed_retweets.json processed (0.42308807373046875 sec)\n",
      "(29%): 2300000 lines in ../data/14-nov/parsed_retweets.json processed (1.1691839694976807 sec)\n",
      "(30%): 2400000 lines in ../data/14-nov/parsed_retweets.json processed (0.45821714401245117 sec)\n",
      "(31%): 2500000 lines in ../data/14-nov/parsed_retweets.json processed (0.380443811416626 sec)\n",
      "(32%): 2600000 lines in ../data/14-nov/parsed_retweets.json processed (0.37580108642578125 sec)\n",
      "(34%): 2700000 lines in ../data/14-nov/parsed_retweets.json processed (0.5851540565490723 sec)\n",
      "(35%): 2800000 lines in ../data/14-nov/parsed_retweets.json processed (0.4482119083404541 sec)\n",
      "(36%): 2900000 lines in ../data/14-nov/parsed_retweets.json processed (0.7885837554931641 sec)\n",
      "(37%): 3000000 lines in ../data/14-nov/parsed_retweets.json processed (0.44202113151550293 sec)\n",
      "(39%): 3100000 lines in ../data/14-nov/parsed_retweets.json processed (0.41396093368530273 sec)\n",
      "(40%): 3200000 lines in ../data/14-nov/parsed_retweets.json processed (0.4081251621246338 sec)\n",
      "(41%): 3300000 lines in ../data/14-nov/parsed_retweets.json processed (0.5038797855377197 sec)\n",
      "(42%): 3400000 lines in ../data/14-nov/parsed_retweets.json processed (0.4109020233154297 sec)\n",
      "(44%): 3500000 lines in ../data/14-nov/parsed_retweets.json processed (0.41083502769470215 sec)\n",
      "(45%): 3600000 lines in ../data/14-nov/parsed_retweets.json processed (0.37885499000549316 sec)\n",
      "(46%): 3700000 lines in ../data/14-nov/parsed_retweets.json processed (0.4780700206756592 sec)\n",
      "(47%): 3800000 lines in ../data/14-nov/parsed_retweets.json processed (0.4350762367248535 sec)\n",
      "(48%): 3900000 lines in ../data/14-nov/parsed_retweets.json processed (0.4304180145263672 sec)\n",
      "(50%): 4000000 lines in ../data/14-nov/parsed_retweets.json processed (0.49993085861206055 sec)\n",
      "(51%): 4100000 lines in ../data/14-nov/parsed_retweets.json processed (0.4129040241241455 sec)\n",
      "(52%): 4200000 lines in ../data/14-nov/parsed_retweets.json processed (0.5012710094451904 sec)\n",
      "(53%): 4300000 lines in ../data/14-nov/parsed_retweets.json processed (0.3830080032348633 sec)\n",
      "(55%): 4400000 lines in ../data/14-nov/parsed_retweets.json processed (0.5244789123535156 sec)\n",
      "(56%): 4500000 lines in ../data/14-nov/parsed_retweets.json processed (0.4504241943359375 sec)\n",
      "(57%): 4600000 lines in ../data/14-nov/parsed_retweets.json processed (0.5199270248413086 sec)\n",
      "(58%): 4700000 lines in ../data/14-nov/parsed_retweets.json processed (0.41339898109436035 sec)\n",
      "(60%): 4800000 lines in ../data/14-nov/parsed_retweets.json processed (0.432236909866333 sec)\n",
      "(61%): 4900000 lines in ../data/14-nov/parsed_retweets.json processed (0.5813000202178955 sec)\n",
      "(62%): 5000000 lines in ../data/14-nov/parsed_retweets.json processed (0.790686845779419 sec)\n",
      "(63%): 5100000 lines in ../data/14-nov/parsed_retweets.json processed (0.43240785598754883 sec)\n",
      "(65%): 5200000 lines in ../data/14-nov/parsed_retweets.json processed (0.5368220806121826 sec)\n",
      "(66%): 5300000 lines in ../data/14-nov/parsed_retweets.json processed (0.4237849712371826 sec)\n",
      "(67%): 5400000 lines in ../data/14-nov/parsed_retweets.json processed (0.5746579170227051 sec)\n",
      "(68%): 5500000 lines in ../data/14-nov/parsed_retweets.json processed (0.3886139392852783 sec)\n",
      "(70%): 5600000 lines in ../data/14-nov/parsed_retweets.json processed (0.5002579689025879 sec)\n",
      "(71%): 5700000 lines in ../data/14-nov/parsed_retweets.json processed (0.37674927711486816 sec)\n",
      "(72%): 5800000 lines in ../data/14-nov/parsed_retweets.json processed (0.4503819942474365 sec)\n",
      "(73%): 5900000 lines in ../data/14-nov/parsed_retweets.json processed (0.4145820140838623 sec)\n",
      "(75%): 6000000 lines in ../data/14-nov/parsed_retweets.json processed (0.5119209289550781 sec)\n",
      "(76%): 6100000 lines in ../data/14-nov/parsed_retweets.json processed (0.40732789039611816 sec)\n",
      "(77%): 6200000 lines in ../data/14-nov/parsed_retweets.json processed (0.38242387771606445 sec)\n",
      "(78%): 6300000 lines in ../data/14-nov/parsed_retweets.json processed (0.5091509819030762 sec)\n",
      "(80%): 6400000 lines in ../data/14-nov/parsed_retweets.json processed (0.3584010601043701 sec)\n",
      "(81%): 6500000 lines in ../data/14-nov/parsed_retweets.json processed (0.43346619606018066 sec)\n",
      "(82%): 6600000 lines in ../data/14-nov/parsed_retweets.json processed (0.4371681213378906 sec)\n",
      "(83%): 6700000 lines in ../data/14-nov/parsed_retweets.json processed (0.47801804542541504 sec)\n",
      "(85%): 6800000 lines in ../data/14-nov/parsed_retweets.json processed (0.39805078506469727 sec)\n",
      "(86%): 6900000 lines in ../data/14-nov/parsed_retweets.json processed (0.37409400939941406 sec)\n",
      "(87%): 7000000 lines in ../data/14-nov/parsed_retweets.json processed (0.4748690128326416 sec)\n",
      "(88%): 7100000 lines in ../data/14-nov/parsed_retweets.json processed (0.38982510566711426 sec)\n",
      "(89%): 7200000 lines in ../data/14-nov/parsed_retweets.json processed (0.49776721000671387 sec)\n",
      "(91%): 7300000 lines in ../data/14-nov/parsed_retweets.json processed (0.3964271545410156 sec)\n",
      "(92%): 7400000 lines in ../data/14-nov/parsed_retweets.json processed (0.4250309467315674 sec)\n",
      "(93%): 7500000 lines in ../data/14-nov/parsed_retweets.json processed (0.4448561668395996 sec)\n",
      "(94%): 7600000 lines in ../data/14-nov/parsed_retweets.json processed (0.3807961940765381 sec)\n",
      "(96%): 7700000 lines in ../data/14-nov/parsed_retweets.json processed (0.4934811592102051 sec)\n",
      "(97%): 7800000 lines in ../data/14-nov/parsed_retweets.json processed (0.38570523262023926 sec)\n",
      "(98%): 7900000 lines in ../data/14-nov/parsed_retweets.json processed (0.4879570007324219 sec)\n",
      "(99%): 8000000 lines in ../data/14-nov/parsed_retweets.json processed (0.46776771545410156 sec)\n",
      "Done loading ../data/14-nov/parsed_retweets.json\n",
      "8044982 lines in ../data/14-nov/parsed_retweets.json processed (37.74600791931152 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8044982 entries, 0 to 8044981\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   timestamp           object\n",
      " 1   user                object\n",
      " 2   retweeted           object\n",
      " 3   retweetedFrom_user  object\n",
      "dtypes: object(4)\n",
      "memory usage: 245.5+ MB\n"
     ]
    }
   ],
   "source": [
    "retweet_df = load_parsed_data('../data/14-nov/parsed_retweets.json')\n",
    "\n",
    "retweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 806800 json lines\n",
      "(12%): 100000 lines in ../data/14-nov/parsed_users.json processed (1.343425989151001 sec)\n",
      "(25%): 200000 lines in ../data/14-nov/parsed_users.json processed (1.1430909633636475 sec)\n",
      "(37%): 300000 lines in ../data/14-nov/parsed_users.json processed (1.0825738906860352 sec)\n",
      "(50%): 400000 lines in ../data/14-nov/parsed_users.json processed (1.0962812900543213 sec)\n",
      "(62%): 500000 lines in ../data/14-nov/parsed_users.json processed (1.0596449375152588 sec)\n",
      "(74%): 600000 lines in ../data/14-nov/parsed_users.json processed (1.1406810283660889 sec)\n",
      "(87%): 700000 lines in ../data/14-nov/parsed_users.json processed (1.1018080711364746 sec)\n",
      "(99%): 800000 lines in ../data/14-nov/parsed_users.json processed (1.0810511112213135 sec)\n",
      "Done loading ../data/14-nov/parsed_users.json\n",
      "806800 lines in ../data/14-nov/parsed_users.json processed (9.147326946258545 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 806800 entries, 1199502560928907265 to 147207892\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   created_at       806800 non-null  object\n",
      " 1   followed_cnts    806800 non-null  int32 \n",
      " 2   friends_count    806800 non-null  int32 \n",
      " 3   protected        806800 non-null  bool  \n",
      " 4   url              149476 non-null  object\n",
      " 5   followers_count  806800 non-null  int32 \n",
      " 6   name             806800 non-null  object\n",
      " 7   handle           806800 non-null  object\n",
      " 8   location         487839 non-null  object\n",
      " 9   verified         806800 non-null  bool  \n",
      " 10  friends          0 non-null       object\n",
      "dtypes: bool(2), int32(3), object(6)\n",
      "memory usage: 53.9+ MB\n"
     ]
    }
   ],
   "source": [
    "cast_cols = {\n",
    "    \"followed_cnts\": \"int32\",\n",
    "    \"friends_count\": \"int32\",\n",
    "    \"followers_count\": \"int32\" \n",
    "}\n",
    "\n",
    "user_df = load_parsed_data('../data/14-nov/parsed_users.json', cast_cols=cast_cols, exclude_cols={\"description\"}, index_col=\"datastore_id\")\n",
    "\n",
    "user_df.info()"
   ]
  },
  {
   "source": [
    "# Build Retweet graph\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "def add_user_to_graph(graph, user_id):\n",
    "    graph.add_node(\n",
    "        user_id,\n",
    "        label=user_df.at[user_id, \"handle\"],\n",
    "        followers=user_df.at[user_id, \"followers_count\"]\n",
    "    )\n",
    "\n",
    "def build_graph(N = 10000, directed=False):\n",
    "    graph = nx.DiGraph() if directed else nx.Graph()\n",
    "    print(\"Building graph...\")\n",
    "    known = 0\n",
    "    unknown = 0\n",
    "    dup_edges = set()\n",
    "    dups = 0\n",
    "    i = 0\n",
    "\n",
    "    retweet_rows = zip(retweet_df[\"retweetedFrom_user\"][:N], retweet_df[\"user\"])\n",
    "\n",
    "    for retweet_author, retweet_user in retweet_rows:\n",
    "        if not graph.has_node(retweet_author):\n",
    "            if retweet_author in user_df.index:\n",
    "                add_user_to_graph(graph, retweet_author)\n",
    "                known += 1\n",
    "            else:\n",
    "                graph.add_node(retweet_author)\n",
    "                unknown += 1\n",
    "        if not graph.has_node(retweet_user):\n",
    "            if retweet_user in user_df.index:\n",
    "                add_user_to_graph(graph, retweet_user)\n",
    "                known += 1\n",
    "            else:\n",
    "                graph.add_node(retweet_user)\n",
    "                unknown += 1\n",
    "        dup_key = str(retweet_user) + str(retweet_author)\n",
    "        if (dup_key not in dup_edges):\n",
    "            graph.add_edge(retweet_user, retweet_author)\n",
    "            dup_edges.add(dup_key)\n",
    "        else: \n",
    "            dups += 1\n",
    "        \n",
    "        if (N > 200000 and i % 100000 == 0):\n",
    "            print(\"{}/{}\".format(i, N))\n",
    "        i += 1\n",
    "\n",
    "    print(\"{} known users\".format(known))\n",
    "    print(\"{} unknown users\".format(unknown))\n",
    "    print(\"{} duplicate edges\".format(dups))\n",
    "    print(\"Done building graph\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdlib.algorithms import infomap\n",
    "def get_largest_subgraph(graph):\n",
    "    connected_components = [graph.subgraph(c) for c in nx.connected_components(graph)]\n",
    "    return max(connected_components, key=len)\n",
    "\n",
    "def print_graph_stats(graph):\n",
    "    print(f\"There are {graph.number_of_nodes()} nodes and {graph.number_of_edges()} \\\n",
    "edges in the Graph\")\n",
    "\n",
    "def detect_communities(graph):\n",
    "    clustering = infomap(graph)\n",
    "    print(\"Found {} communities\".format(len(clustering.communities)))\n",
    "    return clustering.to_node_community_map(), clustering.communities\n",
    "\n",
    "\n",
    "def set_node_community(G, communities):\n",
    "    '''Add community to node attributes'''\n",
    "    for c, v_c in enumerate(communities):\n",
    "        for v in v_c:\n",
    "            # Add 1 to save 0 for external edges\n",
    "            G.nodes[v]['community'] = c + 1\n",
    "\n",
    "def set_edge_community(G):\n",
    "    '''Find internal edges and add their community to their attributes'''\n",
    "    for v, w, in G.edges:\n",
    "        if G.nodes[v]['community'] == G.nodes[w]['community']:\n",
    "            # Internal edge, mark with community\n",
    "            G.edges[v, w]['community'] = G.nodes[v]['community']\n",
    "        else:\n",
    "            # External edge, mark as 0\n",
    "            G.edges[v, w]['community'] = 0\n",
    "\n",
    "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
    "    '''Assign a color to a vertex.'''\n",
    "    r0, g0, b0 = 0, 0, 0\n",
    "    n = 16\n",
    "    low, high = 0.1, 0.9\n",
    "    span = high - low\n",
    "    r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
    "    g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
    "    b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
    "    return (r, g, b)\n",
    "\n",
    "\n",
    "\n",
    "def add_communities_to_graph(graph, communities):\n",
    "    set_node_community(graph, communities)\n",
    "    set_edge_community(graph)\n",
    "\n",
    "def plot_graph_with_communities(graph):\n",
    "    '''Should only be done for a smaller graph'''\n",
    "    plt.rcParams.update(plt.rcParamsDefault)\n",
    "    plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "    plt.style.use('default')\n",
    "    pos = nx.spring_layout(graph, k=0.01, iterations=50)\n",
    "\n",
    "    # Set community color for internal edges\n",
    "    external = [(v, w) for v, w in graph.edges if graph.edges[v, w]['community'] == 0]\n",
    "    internal = [(v, w) for v, w in graph.edges if graph.edges[v, w]['community'] > 0]\n",
    "    internal_color = [\"black\" for e in internal]\n",
    "    node_color = [get_color(graph.nodes[v]['community']) for v in graph.nodes]\n",
    "    # external edges\n",
    "    nx.draw_networkx(\n",
    "        graph, \n",
    "        pos=pos, \n",
    "        node_size=0, \n",
    "        edgelist=external, \n",
    "        edge_color=\"silver\",\n",
    "        node_color=node_color,\n",
    "        alpha=0.8, \n",
    "        with_labels=False)\n",
    "    # internal edges\n",
    "    nx.draw_networkx(\n",
    "        graph, pos=pos, \n",
    "        edgelist=internal, \n",
    "        edge_color=internal_color,\n",
    "        node_color=node_color,\n",
    "        alpha=0.05, \n",
    "        with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building graph...\n",
      "284461 known users\n",
      "61853 unknown users\n",
      "34213 duplicate edges\n",
      "Done building graph\n",
      "There are 53598 nodes and 165787 edges in the Graph\n",
      "...largest subgraph\n",
      "There are 48909 nodes and 163129 edges in the Graph\n"
     ]
    }
   ],
   "source": [
    "large_graph = build_graph(200000)\n",
    "undirected = large_graph.to_undirected()\n",
    "print_graph_stats(large_graph)\n",
    "print(\"...largest subgraph\")\n",
    "largest_subgraph = get_largest_subgraph(undirected)\n",
    "print_graph_stats(largest_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_communities_to_graph(largest_subgraph, communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 46522 nodes and 159670 edges in the Graph\n"
     ]
    }
   ],
   "source": [
    "# Filter down graph to two largest communities\n",
    "community_graph = largest_subgraph.copy()\n",
    "for node, v in community_map.items():\n",
    "    if (v[0] > 1):\n",
    "        community_graph.remove_node(node)\n",
    "\n",
    "print_graph_stats(community_graph)"
   ]
  },
  {
   "source": [
    "# Export Partial Graph for Gephi Drawing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exported to ../data/graphs/graph-with-communities.gexf\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/graphs/graph-with-communities.gexf\"\n",
    "nx.write_gexf(community_graph, filename)\n",
    "print(\"Exported to {}\".format(filename))"
   ]
  },
  {
   "source": [
    "# Cluster Full Graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building graph out of 8044982 retweets\n",
      "Building graph...\n",
      "0/8044982\n",
      "100000/8044982\n",
      "200000/8044982\n",
      "300000/8044982\n",
      "400000/8044982\n",
      "500000/8044982\n",
      "600000/8044982\n",
      "700000/8044982\n",
      "800000/8044982\n",
      "900000/8044982\n",
      "1000000/8044982\n",
      "1100000/8044982\n",
      "1200000/8044982\n",
      "1300000/8044982\n",
      "1400000/8044982\n",
      "1500000/8044982\n",
      "1600000/8044982\n",
      "1700000/8044982\n",
      "1800000/8044982\n",
      "1900000/8044982\n",
      "2000000/8044982\n",
      "2100000/8044982\n",
      "2200000/8044982\n",
      "2300000/8044982\n",
      "2400000/8044982\n",
      "2500000/8044982\n",
      "2600000/8044982\n",
      "2700000/8044982\n",
      "2800000/8044982\n",
      "2900000/8044982\n",
      "3000000/8044982\n",
      "3100000/8044982\n",
      "3200000/8044982\n",
      "3300000/8044982\n",
      "3400000/8044982\n",
      "3500000/8044982\n",
      "3600000/8044982\n",
      "3700000/8044982\n",
      "3800000/8044982\n",
      "3900000/8044982\n",
      "4000000/8044982\n",
      "4100000/8044982\n",
      "4200000/8044982\n",
      "4300000/8044982\n",
      "4400000/8044982\n",
      "4500000/8044982\n",
      "4600000/8044982\n",
      "4700000/8044982\n",
      "4800000/8044982\n",
      "4900000/8044982\n",
      "5000000/8044982\n",
      "5100000/8044982\n",
      "5200000/8044982\n",
      "5300000/8044982\n",
      "5400000/8044982\n",
      "5500000/8044982\n",
      "5600000/8044982\n",
      "5700000/8044982\n",
      "5800000/8044982\n",
      "5900000/8044982\n",
      "6000000/8044982\n",
      "6100000/8044982\n",
      "6200000/8044982\n",
      "6300000/8044982\n",
      "6400000/8044982\n",
      "6500000/8044982\n",
      "6600000/8044982\n",
      "6700000/8044982\n",
      "6800000/8044982\n",
      "6900000/8044982\n",
      "7000000/8044982\n",
      "7100000/8044982\n",
      "7200000/8044982\n",
      "7300000/8044982\n",
      "7400000/8044982\n",
      "7500000/8044982\n",
      "7600000/8044982\n",
      "7700000/8044982\n",
      "7800000/8044982\n",
      "7900000/8044982\n",
      "8000000/8044982\n",
      "407882 known users\n",
      "913704 unknown users\n",
      "1351149 duplicate edges\n",
      "Done building graph\n",
      "There are 1321586 nodes and 6678842 edges in the Graph\n"
     ]
    }
   ],
   "source": [
    "size = retweet_df.shape[0]\n",
    "print(\"Building graph out of {} retweets\".format(size))\n",
    "full_graph = build_graph(size)\n",
    "print_graph_stats(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...largest subgraph\n",
      "Found 126 communities\n",
      "Community distribution\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[645191,\n",
       " 560052,\n",
       " 22539,\n",
       " 18045,\n",
       " 11377,\n",
       " 4353,\n",
       " 4018,\n",
       " 3889,\n",
       " 3736,\n",
       " 2754,\n",
       " 1417,\n",
       " 1250,\n",
       " 1154,\n",
       " 1154,\n",
       " 1106,\n",
       " 812,\n",
       " 646,\n",
       " 443,\n",
       " 438,\n",
       " 416,\n",
       " 359,\n",
       " 342,\n",
       " 337,\n",
       " 280,\n",
       " 264,\n",
       " 263,\n",
       " 208,\n",
       " 192,\n",
       " 182,\n",
       " 176,\n",
       " 170,\n",
       " 153,\n",
       " 142,\n",
       " 142,\n",
       " 136,\n",
       " 128,\n",
       " 121,\n",
       " 118,\n",
       " 114,\n",
       " 105,\n",
       " 99,\n",
       " 93,\n",
       " 87,\n",
       " 84,\n",
       " 84,\n",
       " 79,\n",
       " 78,\n",
       " 77,\n",
       " 72,\n",
       " 65,\n",
       " 61,\n",
       " 60,\n",
       " 59,\n",
       " 59,\n",
       " 58,\n",
       " 57,\n",
       " 53,\n",
       " 52,\n",
       " 51,\n",
       " 49,\n",
       " 49,\n",
       " 48,\n",
       " 44,\n",
       " 40,\n",
       " 40,\n",
       " 38,\n",
       " 36,\n",
       " 34,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 26,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 23,\n",
       " 22,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1]"
      ]
     },
     "metadata": {},
     "execution_count": 356
    }
   ],
   "source": [
    "print(\"...largest subgraph\")\n",
    "largest_subgraph_community_map, largest_subgraph_communities = detect_communities(largest_subgraph)\n",
    "print(\"Community distribution\")\n",
    "[len(c) for c in communities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 1321586 nodes and 6678842 edges in the Graph\n"
     ]
    }
   ],
   "source": [
    "print_graph_stats(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 13887 connected components\n"
     ]
    }
   ],
   "source": [
    "connected_components = [full_graph.subgraph(c) for c in nx.connected_components(full_graph)]\n",
    "print(\"There are {} connected components\".format(len(connected_components)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 12630 communities\n"
     ]
    }
   ],
   "source": [
    "full_graph_community_map, full_graph_communities = detect_communities(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...largest subgraph\n",
      "There are 1291086 nodes and 6660681 edges in the Graph\n"
     ]
    }
   ],
   "source": [
    "print(\"...largest subgraph\")\n",
    "largest_subgraph = get_largest_subgraph(full_graph)\n",
    "print_graph_stats(largest_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_communities_to_graph(largest_subgraph, largest_subgraph_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exported to ../data/graphs/full-graph.gexf\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/graphs/full-graph.gexf\"\n",
    "nx.write_gexf(full_graph, filename)\n",
    "print(\"Exported to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exported to ../data/graphs/full-connected-community-graph.gexf\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/graphs/full-connected-community-graph.gexf\"\n",
    "nx.write_gexf(largest_subgraph, filename)\n",
    "print(\"Exported to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_with_clustering = user_df.copy()\n",
    "\n",
    "def get_cluster(user_id):\n",
    "    if str(user_id) in largest_subgraph_community_map and len(largest_subgraph_community_map[user_id]) > 0:\n",
    "        return community_map[str(user_id)][0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "user_df_with_clustering[\"infomap_cluster\"] = user_df_with_clustering.apply(lambda x: get_cluster(x.name), axis=1)"
   ]
  },
  {
   "source": [
    "## Export dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(806800, 12)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NaN      414331\n",
       "0.0      257148\n",
       "1.0      122868\n",
       "3.0        3451\n",
       "2.0        2385\n",
       "          ...  \n",
       "53.0          2\n",
       "98.0          2\n",
       "94.0          2\n",
       "44.0          2\n",
       "107.0         1\n",
       "Name: infomap_cluster, Length: 114, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "print(user_df_with_clustering.shape)\n",
    "user_df_with_clustering[\"infomap_cluster\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_with_clustering.to_csv(\"../data/dataframes/user_df_with_clustering.csv\", index_label=\"datastore_id\")"
   ]
  }
 ]
}