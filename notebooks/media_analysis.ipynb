{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('voter-fraud': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a0f90dcbb54bea4e60f894f8fd1d686cc0f74395b4029405cc9f13e0b975e641"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import setup, load_tweet_df, load_media_df\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 2696807 json lines\n",
      "(4%): 100000 lines in ../data/14-nov/parsed_tweets.json processed (2.2798409461975098 sec)\n",
      "(7%): 200000 lines in ../data/14-nov/parsed_tweets.json processed (2.299914836883545 sec)\n",
      "(11%): 300000 lines in ../data/14-nov/parsed_tweets.json processed (2.6531641483306885 sec)\n",
      "(15%): 400000 lines in ../data/14-nov/parsed_tweets.json processed (2.378187894821167 sec)\n",
      "(19%): 500000 lines in ../data/14-nov/parsed_tweets.json processed (2.5499980449676514 sec)\n",
      "(22%): 600000 lines in ../data/14-nov/parsed_tweets.json processed (2.8550190925598145 sec)\n",
      "(26%): 700000 lines in ../data/14-nov/parsed_tweets.json processed (3.1365549564361572 sec)\n",
      "(30%): 800000 lines in ../data/14-nov/parsed_tweets.json processed (1.7076022624969482 sec)\n",
      "(33%): 900000 lines in ../data/14-nov/parsed_tweets.json processed (3.5984036922454834 sec)\n",
      "(37%): 1000000 lines in ../data/14-nov/parsed_tweets.json processed (1.8618502616882324 sec)\n",
      "(41%): 1100000 lines in ../data/14-nov/parsed_tweets.json processed (2.037889003753662 sec)\n",
      "(44%): 1200000 lines in ../data/14-nov/parsed_tweets.json processed (3.9089107513427734 sec)\n",
      "(48%): 1300000 lines in ../data/14-nov/parsed_tweets.json processed (1.6318981647491455 sec)\n",
      "(52%): 1400000 lines in ../data/14-nov/parsed_tweets.json processed (1.6777739524841309 sec)\n",
      "(56%): 1500000 lines in ../data/14-nov/parsed_tweets.json processed (4.471527099609375 sec)\n",
      "(59%): 1600000 lines in ../data/14-nov/parsed_tweets.json processed (1.6132137775421143 sec)\n",
      "(63%): 1700000 lines in ../data/14-nov/parsed_tweets.json processed (1.8002691268920898 sec)\n",
      "(67%): 1800000 lines in ../data/14-nov/parsed_tweets.json processed (5.149240255355835 sec)\n",
      "(70%): 1900000 lines in ../data/14-nov/parsed_tweets.json processed (1.7049059867858887 sec)\n",
      "(74%): 2000000 lines in ../data/14-nov/parsed_tweets.json processed (1.696890115737915 sec)\n",
      "(78%): 2100000 lines in ../data/14-nov/parsed_tweets.json processed (1.6795401573181152 sec)\n",
      "(82%): 2200000 lines in ../data/14-nov/parsed_tweets.json processed (1.7052853107452393 sec)\n",
      "(85%): 2300000 lines in ../data/14-nov/parsed_tweets.json processed (5.939496994018555 sec)\n",
      "(89%): 2400000 lines in ../data/14-nov/parsed_tweets.json processed (1.7113540172576904 sec)\n",
      "(93%): 2500000 lines in ../data/14-nov/parsed_tweets.json processed (1.7611088752746582 sec)\n",
      "(96%): 2600000 lines in ../data/14-nov/parsed_tweets.json processed (1.695106029510498 sec)\n",
      "Done loading ../data/14-nov/parsed_tweets.json\n",
      "2696807 lines in ../data/14-nov/parsed_tweets.json processed (67.12725114822388 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2696807 entries, 0 to 2696806\n",
      "Data columns (total 45 columns):\n",
      " #   Column                      Dtype          \n",
      "---  ------                      -----          \n",
      " 0   datastore_id                object         \n",
      " 1   hashtags                    object         \n",
      " 2   urls                        object         \n",
      " 3   hasMedia                    bool           \n",
      " 4   quote_tweet                 object         \n",
      " 5   retweet_count               int32          \n",
      " 6   timestamp                   object         \n",
      " 7   quote_count                 int32          \n",
      " 8   user                        object         \n",
      " 9   tokens                      object         \n",
      " 10  voter fraud                 Sparse[int8, 0]\n",
      " 11  #electionfraud              Sparse[int8, 0]\n",
      " 12  ballot fraud                Sparse[int8, 0]\n",
      " 13  #voterfraud                 Sparse[int8, 0]\n",
      " 14  #stopthesteal               Sparse[int8, 0]\n",
      " 15  stolen ballots              Sparse[int8, 0]\n",
      " 16  election fraud              Sparse[int8, 0]\n",
      " 17  ballot harvesting           Sparse[int8, 0]\n",
      " 18  #electioninterference       Sparse[int8, 0]\n",
      " 19  #cheatingdemocrats          Sparse[int8, 0]\n",
      " 20  election interference       Sparse[int8, 0]\n",
      " 21  #ballotfraud                Sparse[int8, 0]\n",
      " 22  cheating democrats          Sparse[int8, 0]\n",
      " 23  #electiontampering          Sparse[int8, 0]\n",
      " 24  #ballotharvesting           Sparse[int8, 0]\n",
      " 25  #voterfraudisreal           Sparse[int8, 0]\n",
      " 26  destroyed ballots           Sparse[int8, 0]\n",
      " 27  democrats cheat             Sparse[int8, 0]\n",
      " 28  #stopvoterfraud             Sparse[int8, 0]\n",
      " 29  #ballotvoterfraud           Sparse[int8, 0]\n",
      " 30  election tampering          Sparse[int8, 0]\n",
      " 31  #democratvoterfraud         Sparse[int8, 0]\n",
      " 32  discarded ballots           Sparse[int8, 0]\n",
      " 33  vote by mail fraud          Sparse[int8, 0]\n",
      " 34  harvest ballot              Sparse[int8, 0]\n",
      " 35  #gopvoterfraud              Sparse[int8, 0]\n",
      " 36  #nomailinvoting             Sparse[int8, 0]\n",
      " 37  #votebymailfraud            Sparse[int8, 0]\n",
      " 38  #mailinvoterfraud           Sparse[int8, 0]\n",
      " 39  pre-filled ballot           Sparse[int8, 0]\n",
      " 40  hacked voting machine       Sparse[int8, 0]\n",
      " 41  #ilhanomarballotharvesting  Sparse[int8, 0]\n",
      " 42  #ilhanomarvoterfraud        Sparse[int8, 0]\n",
      " 43  #hackedvotingmachines       Sparse[int8, 0]\n",
      " 44  #discardedballots           Sparse[int8, 0]\n",
      "dtypes: Sparse[int8, 0](35), bool(1), int32(2), object(7)\n",
      "memory usage: 173.9+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet_df, recent_tweet_df = load_tweet_df()\n",
    "\n",
    "tweet_df.info()"
   ]
  },
  {
   "source": [
    "# Top URLs in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "url_map = defaultdict(lambda: {\n",
    "    \"tweet_ids\": set(),\n",
    "    \"aggregated_retweet_count\": 0,\n",
    "    \"aggregated_quote_count\": 0\n",
    "})\n",
    "\n",
    "total_tweet_count = 0\n",
    "total_retweet_count = 0\n",
    "total_quote_count = 0\n",
    "\n",
    "for i, tweet_id, urls, retweet_count, quote_count in recent_tweet_df[[\"datastore_id\", \"urls\", \"retweet_count\", \"quote_count\"]].itertuples():\n",
    "    has_relevant_url = False\n",
    "    for url in urls:\n",
    "        if \"twitter.com/\" not in url:\n",
    "            url = url.lower()\n",
    "            has_relevant_url = True\n",
    "            url_map[url][\"tweet_ids\"].add(tweet_id)\n",
    "            url_map[url][\"aggregated_retweet_count\"] += retweet_count\n",
    "            url_map[url][\"aggregated_quote_count\"] += quote_count\n",
    "            total_retweet_count += retweet_count\n",
    "            total_quote_count += quote_count\n",
    "    if has_relevant_url:\n",
    "        total_tweet_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of tweets with URLs (excluding twitter.com URLs): 214,666\nUnique URLs shared: 58,529\nURL share retweet count: 1,354,021\nURL share quote count: 171,010\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets with URLs (excluding twitter.com URLs): {:,}\".format(total_tweet_count))\n",
    "print(\"Unique URLs shared: {:,}\".format(len(url_map.keys())))\n",
    "print(\"URL share retweet count: {:,}\".format(total_retweet_count))\n",
    "print(\"URL share quote count: {:,}\".format(total_quote_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top URLs in the dataset:\n267885 retweets from 3106 tweets - https://thefederalist.com/2020/11/05/democrats-have-been-denying-trump-the-presidency-ever-since-his-first-victory/#.x6riz55se8q.twitter\n157701 retweets from 26 tweets - https://hann.it/2uab4bp\n153837 retweets from 3184 tweets - https://www.breitbart.com/2020-election/2020/11/04/report-trump-campaign-assembling-all-star-legal-team-election-challenges/\n141187 retweets from 29623 tweets - https://youtu.be/p3ss8m1_3ke\n134195 retweets from 7827 tweets - https://www.thegatewaypundit.com/2020/11/watch-suitcases-coolers-rolled-detroit-voting-center-4-brought-secure-counting-area/\n132086 retweets from 1072 tweets - https://justthenews.com/politics-policy/elections/yes-america-there-voter-fraud-these-recent-cases-prove-it\n94226 retweets from 2578 tweets - https://nypost.com/2020/08/29/political-insider-explains-voter-fraud-with-mail-in-ballots/\n89205 retweets from 3965 tweets - https://www.nytimes.com/2020/11/04/technology/sharpies-ballots-arizona.html\n87089 retweets from 2599 tweets - https://www.zerohedge.com/political/facebook-removes-fast-growing-pro-trump-group-organized-vote-counting-protests\n85650 retweets from 219 tweets - http://ow.ly/njbo30rifoj\n"
     ]
    }
   ],
   "source": [
    "def top_urls_by_retweet_count(url_map, N = 10):\n",
    "    for url in heapq.nlargest(N, url_map, key=lambda x: url_map.get(x)[\"aggregated_retweet_count\"]):\n",
    "        url_stats = url_map.get(url)\n",
    "        tweet_count = len(url_stats[\"tweet_ids\"])\n",
    "        retweet_count = url_stats[\"aggregated_retweet_count\"]\n",
    "        print(\"{} retweets from {} tweets - {}\".format(retweet_count, tweet_count, url))\n",
    "\n",
    "def transform_url_map(url_map, filter_fn=lambda x: x, map_key=lambda x: x):\n",
    "    new_map = {}\n",
    "    for key, val in url_map.items():\n",
    "        if filter_fn(key):\n",
    "            mapped_key = map_key(key)\n",
    "            if (mapped_key in new_map):\n",
    "                existing_entry = new_map[mapped_key]\n",
    "                existing_entry[\"tweet_ids\"].update(val[\"tweet_ids\"])\n",
    "                existing_entry[\"aggregated_retweet_count\"] += val[\"aggregated_retweet_count\"]\n",
    "                existing_entry[\"aggregated_quote_count\"] += val[\"aggregated_quote_count\"]\n",
    "            else:\n",
    "                new_map[mapped_key] = val\n",
    "\n",
    "    return new_map\n",
    "\n",
    "print(\"Top URLs in the dataset:\")\n",
    "top_urls_by_retweet_count(transform_url_map(url_map))\n"
   ]
  },
  {
   "source": [
    "## Top Domains in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique domains in the dataset: 7,276\nTop domains in the dataset:\n178590 retweets from 3106 tweets - thefederalist.com\n106272 retweets from 26 tweets - hann.it\n102689 retweets from 3184 tweets - breitbart.com\n89470 retweets from 7826 tweets - thegatewaypundit.com\n88141 retweets from 1070 tweets - justthenews.com\n86985 retweets from 29623 tweets - youtu.be\n67178 retweets from 2578 tweets - nypost.com\n59475 retweets from 3965 tweets - nytimes.com\n58170 retweets from 2599 tweets - zerohedge.com\n57100 retweets from 219 tweets - ow.ly\n"
     ]
    }
   ],
   "source": [
    "def map_to_domain(url):\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    return parsed.netloc.replace(\"www.\", \"\")\n",
    "\n",
    "domain_stats_map = transform_url_map(url_map, map_key=map_to_domain)\n",
    "print(\"Unique domains in the dataset: {:,}\".format(len(domain_url_map.keys())))\n",
    "print(\"Top domains in the dataset:\")\n",
    "top_urls_by_retweet_count(domain_url_map)"
   ]
  },
  {
   "source": [
    "## Top YouTube URLs in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Deleted URL params after normalizing Youtube URLs {'has_verified', 'redir_token', 'attr_tag', 'bsft_mime_type', 'bsft_clkid', 'utm_source', 'utm_medium', 'bsft_utid', 'view_as', 'form', 'bsft_ek', 'ebc', 'bsft_aaid', 'utm_campaign', 'persist_app', 'reload', 'from', 'utm_content', 'pbjreload', 'amp;feature', 'ab_channel', 'pc', 'authuser', 'time_continue', 'fbclid', 'list', 'start_radio', 'bsft_mid', 'bsft_tv', 'lc', 'v', 'bsft_eid', 'bsft_lx', 'bsft_link_id', 'html_redirect', 'q', 'index', 'autoplay', 't', 'bsft_uid', 'd', 'app', 'feature', 'event', 'noapp', 'search_query'}\n\nUnique Youtube URLs in the dataset: 5,530\nTop Youtube URLs in the dataset:\n202529 retweets from 29623 tweets - https://youtu.be/p3ss8m1_3ke\n65284 retweets from 5997 tweets - https://youtu.be/9vwcjpbnuz0\n15812 retweets from 30 tweets - https://youtu.be/byta1amljxy\n14382 retweets from 336 tweets - https://youtu.be/96-bqaivopc\n7325 retweets from 433 tweets - https://youtu.be/w7vkbipeyz4\n5924 retweets from 1402 tweets - https://youtu.be/qu7zrp0gkbq\n2962 retweets from 13 tweets - https://youtu.be/fveonzpdbiw\n2870 retweets from 508 tweets - https://youtu.be/ztu5y5obwpk\n1953 retweets from 775 tweets - https://youtu.be/ma8a2g6ttp0\n1860 retweets from 13 tweets - https://youtu.be/88jwxic9orm\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import parse_qs, urlencode, urlparse\n",
    "\n",
    "deleted_url_params = set()\n",
    "\n",
    "def detect_youtube_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return \"youtu.be\" in parsed.netloc or \"youtube.com\" in parsed.netloc\n",
    "\n",
    "def normalize_youtube_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    parsed = parsed._replace()\n",
    "    query_params = parse_qs(parsed.query)\n",
    "    if (parsed.path == '/watch' and \"v\" in query_params):\n",
    "        updated_path = '/' + query_params[\"v\"][0]\n",
    "        del query_params[\"v\"]\n",
    "        parsed = parsed._replace(path=updated_path)\n",
    "    deleted_url_params.update(query_params.keys())\n",
    "    query_params = {}\n",
    "    updated_query = urlencode(query_params, doseq=True)\n",
    "    parsed = parsed._replace(scheme='https', netloc='youtu.be', query=updated_query)\n",
    "    return parsed.geturl()\n",
    "\n",
    "youtube_url_map = transform_url_map(\n",
    "    url_map, \n",
    "    filter_fn=detect_youtube_url,\n",
    "    map_key=normalize_youtube_url\n",
    ")\n",
    "\n",
    "print(\"Deleted URL params after normalizing Youtube URLs\", deleted_url_params)\n",
    "\n",
    "print()\n",
    "print(\"Unique Youtube URLs in the dataset: {:,}\".format(len(youtube_url_map.keys())))\n",
    "print(\"Top Youtube URLs in the dataset:\")\n",
    "top_urls_by_retweet_count(youtube_url_map)"
   ]
  },
  {
   "source": [
    "# Top Media in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 156311 json lines\n",
      "(64%): 100000 lines in ../data/14-nov/parsed_media.json processed (0.5224850177764893 sec)\n",
      "Done loading ../data/14-nov/parsed_media.json\n",
      "156311 lines in ../data/14-nov/parsed_media.json processed (0.7744948863983154 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156311 entries, 0 to 156310\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   media_url  156311 non-null  object\n",
      " 1   tweet_id   156311 non-null  object\n",
      " 2   media_id   156311 non-null  object\n",
      " 3   type       156311 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "media_df = load_media_df()\n",
    "media_df.info()"
   ]
  },
  {
   "source": [
    "## Export to JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_sets(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "\n",
    "    return obj\n",
    "\n",
    "with open(\"./data_export/url_stats/youtube_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(youtube_url_map, f, sort_keys=True, indent=2, default=serialize_sets)\n",
    "\n",
    "with open(\"./data_export/url_stats/domains.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(domain_stats_map, f, sort_keys=True, indent=2, default=serialize_sets)\n",
    "\n",
    "with open(\"./data_export/url_stats/all_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(url_map, f, sort_keys=True, indent=2, default=serialize_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}