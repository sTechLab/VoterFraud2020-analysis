{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('voter-fraud': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a0f90dcbb54bea4e60f894f8fd1d686cc0f74395b4029405cc9f13e0b975e641"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import setup, load_tweet_df, load_media_df\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 2696807 json lines\n",
      "(4%): 100000 lines in ../data/14-nov/parsed_tweets.json processed (2.472034215927124 sec)\n",
      "(7%): 200000 lines in ../data/14-nov/parsed_tweets.json processed (2.2573370933532715 sec)\n",
      "(11%): 300000 lines in ../data/14-nov/parsed_tweets.json processed (2.6654880046844482 sec)\n",
      "(15%): 400000 lines in ../data/14-nov/parsed_tweets.json processed (2.3025078773498535 sec)\n",
      "(19%): 500000 lines in ../data/14-nov/parsed_tweets.json processed (2.856652021408081 sec)\n",
      "(22%): 600000 lines in ../data/14-nov/parsed_tweets.json processed (2.842512845993042 sec)\n",
      "(26%): 700000 lines in ../data/14-nov/parsed_tweets.json processed (2.9592249393463135 sec)\n",
      "(30%): 800000 lines in ../data/14-nov/parsed_tweets.json processed (1.809798002243042 sec)\n",
      "(33%): 900000 lines in ../data/14-nov/parsed_tweets.json processed (3.6938552856445312 sec)\n",
      "(37%): 1000000 lines in ../data/14-nov/parsed_tweets.json processed (1.8063840866088867 sec)\n",
      "(41%): 1100000 lines in ../data/14-nov/parsed_tweets.json processed (1.7608168125152588 sec)\n",
      "(44%): 1200000 lines in ../data/14-nov/parsed_tweets.json processed (3.9085099697113037 sec)\n",
      "(48%): 1300000 lines in ../data/14-nov/parsed_tweets.json processed (1.7787621021270752 sec)\n",
      "(52%): 1400000 lines in ../data/14-nov/parsed_tweets.json processed (1.8000900745391846 sec)\n",
      "(56%): 1500000 lines in ../data/14-nov/parsed_tweets.json processed (4.481168746948242 sec)\n",
      "(59%): 1600000 lines in ../data/14-nov/parsed_tweets.json processed (1.8358712196350098 sec)\n",
      "(63%): 1700000 lines in ../data/14-nov/parsed_tweets.json processed (2.035452127456665 sec)\n",
      "(67%): 1800000 lines in ../data/14-nov/parsed_tweets.json processed (5.198636054992676 sec)\n",
      "(70%): 1900000 lines in ../data/14-nov/parsed_tweets.json processed (1.8905150890350342 sec)\n",
      "(74%): 2000000 lines in ../data/14-nov/parsed_tweets.json processed (2.0719451904296875 sec)\n",
      "(78%): 2100000 lines in ../data/14-nov/parsed_tweets.json processed (1.969789981842041 sec)\n",
      "(82%): 2200000 lines in ../data/14-nov/parsed_tweets.json processed (1.8189969062805176 sec)\n",
      "(85%): 2300000 lines in ../data/14-nov/parsed_tweets.json processed (5.804296970367432 sec)\n",
      "(89%): 2400000 lines in ../data/14-nov/parsed_tweets.json processed (2.09301495552063 sec)\n",
      "(93%): 2500000 lines in ../data/14-nov/parsed_tweets.json processed (1.8091588020324707 sec)\n",
      "(96%): 2600000 lines in ../data/14-nov/parsed_tweets.json processed (1.8836538791656494 sec)\n",
      "Done loading ../data/14-nov/parsed_tweets.json\n",
      "2696807 lines in ../data/14-nov/parsed_tweets.json processed (69.4847240447998 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2696807 entries, 1324415392761171970 to 1324026879720984580\n",
      "Data columns (total 44 columns):\n",
      " #   Column                      Dtype          \n",
      "---  ------                      -----          \n",
      " 0   hashtags                    object         \n",
      " 1   urls                        object         \n",
      " 2   hasMedia                    bool           \n",
      " 3   quote_tweet                 object         \n",
      " 4   retweet_count               int32          \n",
      " 5   timestamp                   object         \n",
      " 6   quote_count                 int32          \n",
      " 7   user                        object         \n",
      " 8   tokens                      object         \n",
      " 9   voter fraud                 Sparse[int8, 0]\n",
      " 10  #electionfraud              Sparse[int8, 0]\n",
      " 11  ballot fraud                Sparse[int8, 0]\n",
      " 12  #voterfraud                 Sparse[int8, 0]\n",
      " 13  #stopthesteal               Sparse[int8, 0]\n",
      " 14  stolen ballots              Sparse[int8, 0]\n",
      " 15  election fraud              Sparse[int8, 0]\n",
      " 16  ballot harvesting           Sparse[int8, 0]\n",
      " 17  #electioninterference       Sparse[int8, 0]\n",
      " 18  #cheatingdemocrats          Sparse[int8, 0]\n",
      " 19  election interference       Sparse[int8, 0]\n",
      " 20  #ballotfraud                Sparse[int8, 0]\n",
      " 21  cheating democrats          Sparse[int8, 0]\n",
      " 22  #electiontampering          Sparse[int8, 0]\n",
      " 23  #ballotharvesting           Sparse[int8, 0]\n",
      " 24  #voterfraudisreal           Sparse[int8, 0]\n",
      " 25  destroyed ballots           Sparse[int8, 0]\n",
      " 26  democrats cheat             Sparse[int8, 0]\n",
      " 27  #stopvoterfraud             Sparse[int8, 0]\n",
      " 28  #ballotvoterfraud           Sparse[int8, 0]\n",
      " 29  election tampering          Sparse[int8, 0]\n",
      " 30  #democratvoterfraud         Sparse[int8, 0]\n",
      " 31  discarded ballots           Sparse[int8, 0]\n",
      " 32  vote by mail fraud          Sparse[int8, 0]\n",
      " 33  harvest ballot              Sparse[int8, 0]\n",
      " 34  #gopvoterfraud              Sparse[int8, 0]\n",
      " 35  #nomailinvoting             Sparse[int8, 0]\n",
      " 36  #votebymailfraud            Sparse[int8, 0]\n",
      " 37  #mailinvoterfraud           Sparse[int8, 0]\n",
      " 38  pre-filled ballot           Sparse[int8, 0]\n",
      " 39  hacked voting machine       Sparse[int8, 0]\n",
      " 40  #ilhanomarballotharvesting  Sparse[int8, 0]\n",
      " 41  #ilhanomarvoterfraud        Sparse[int8, 0]\n",
      " 42  #hackedvotingmachines       Sparse[int8, 0]\n",
      " 43  #discardedballots           Sparse[int8, 0]\n",
      "dtypes: Sparse[int8, 0](35), bool(1), int32(2), object(6)\n",
      "memory usage: 173.9+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet_df, recent_tweet_df = load_tweet_df()\n",
    "\n",
    "tweet_df.info()"
   ]
  },
  {
   "source": [
    "# Top URLs in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "url_map = defaultdict(lambda: {\n",
    "    \"tweet_ids\": set(),\n",
    "    \"aggregated_retweet_count\": 0,\n",
    "    \"aggregated_quote_count\": 0\n",
    "})\n",
    "\n",
    "total_tweet_count = 0\n",
    "total_retweet_count = 0\n",
    "total_quote_count = 0\n",
    "\n",
    "for tweet_id, urls, retweet_count, quote_count in recent_tweet_df[[\"urls\", \"retweet_count\", \"quote_count\"]].itertuples():\n",
    "    has_relevant_url = False\n",
    "    for url in urls:\n",
    "        if \"twitter.com/\" not in url:\n",
    "            url = url.lower()\n",
    "            has_relevant_url = True\n",
    "            url_map[url][\"tweet_ids\"].add(tweet_id)\n",
    "            url_map[url][\"aggregated_retweet_count\"] += retweet_count\n",
    "            url_map[url][\"aggregated_quote_count\"] += quote_count\n",
    "            total_retweet_count += retweet_count\n",
    "            total_quote_count += quote_count\n",
    "    if has_relevant_url:\n",
    "        total_tweet_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of tweets with URLs (excluding twitter.com URLs): 214,666\nUnique URLs shared: 58,529\nURL share retweet count: 1,354,021\nURL share quote count: 171,010\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets with URLs (excluding twitter.com URLs): {:,}\".format(total_tweet_count))\n",
    "print(\"Unique URLs shared: {:,}\".format(len(url_map.keys())))\n",
    "print(\"URL share retweet count: {:,}\".format(total_retweet_count))\n",
    "print(\"URL share quote count: {:,}\".format(total_quote_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top URLs in the dataset:\n36770 retweets from 192 tweets - https://www.breitbart.com/2020-election/2020/11/07/republican-led-michigan-legislature-to-hold-hearings-on-election-fraud-claims/\n26685 retweets from 64 tweets - https://thefederalist.com/2020/11/08/america-wont-trust-elections-until-the-voter-fraud-is-investigated/#.x6ihcjhduyr.twitter\n26267 retweets from 204 tweets - http://djt45.co/stopfraud\n21252 retweets from 563 tweets - https://breaking911.com/u-s-postal-worker-caught-at-canadian-border-with-stolen-ballots-in-car-trunk/\n18282 retweets from 138 tweets - https://www.washingtonexaminer.com/news/lindsey-graham-possible-ballot-harvesting-in-pennsylvania-involving-25-000-nursing-home-residents\n16945 retweets from 55 tweets - https://www.houstonchronicle.com/politics/texas/article/texas-lt-gov-dan-patrick-offers-1-million-15716973.php?utm_campaign=cms%20sharing%20tools%20(premium)&utm_source=t.co&utm_medium=referral\n15667 retweets from 3073 tweets - https://www.whitehouse.gov/presidential-actions/executive-order-imposing-certain-sanctions-event-foreign-interference-united-states-election/\n15636 retweets from 84 tweets - https://nypost.com/2020/11/11/usps-whistleblower-denies-wapo-claim-he-recanted-allegations/?utm_source=twitter_sitebuttons&utm_medium=site%20buttons&utm_campaign=site%20buttons\n14123 retweets from 15 tweets - https://www.zerohedge.com/political/30-states-computer-system-known-be-defective-tallying-votes\n13826 retweets from 206 tweets - https://townhall.com/tipsheet/bethbaumann/2020/11/04/usps-whistleblower-in-michigan-claims-higher-ups-were-engaging-in-voter-fraud-n2579501\n"
     ]
    }
   ],
   "source": [
    "def top_urls_by_retweet_count(url_map, N = 10):\n",
    "    for url in heapq.nlargest(N, url_map, key=lambda x: url_map.get(x)[\"aggregated_retweet_count\"]):\n",
    "        url_stats = url_map.get(url)\n",
    "        tweet_count = len(url_stats[\"tweet_ids\"])\n",
    "        retweet_count = url_stats[\"aggregated_retweet_count\"]\n",
    "        print(\"{} retweets from {} tweets - {}\".format(retweet_count, tweet_count, url))\n",
    "\n",
    "def transform_url_map(url_map, filter_fn=lambda x: x, map_key=lambda x: x):\n",
    "    new_map = {}\n",
    "    for key, val in url_map.items():\n",
    "        if filter_fn(key):\n",
    "            mapped_key = map_key(key)\n",
    "            if (mapped_key in new_map):\n",
    "                existing_entry = new_map[mapped_key]\n",
    "                existing_entry[\"tweet_ids\"].update(val[\"tweet_ids\"])\n",
    "                existing_entry[\"aggregated_retweet_count\"] += val[\"aggregated_retweet_count\"]\n",
    "                existing_entry[\"aggregated_quote_count\"] += val[\"aggregated_quote_count\"]\n",
    "                new_map[mapped_key] = existing_entry\n",
    "            else:\n",
    "                new_map[mapped_key] = val.copy()\n",
    "\n",
    "    return new_map\n",
    "\n",
    "print(\"Top URLs in the dataset:\")\n",
    "top_urls_by_retweet_count(url_map)\n"
   ]
  },
  {
   "source": [
    "## Top Domains in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique domains in the dataset: 6,924\nTop domains in the dataset:\n89295 retweets from 3106 tweets - thefederalist.com\n54843 retweets from 26 tweets - hann.it\n51541 retweets from 3184 tweets - breitbart.com\n44745 retweets from 7827 tweets - thegatewaypundit.com\n44196 retweets from 1072 tweets - justthenews.com\n40130 retweets from 2578 tweets - nypost.com\n39923 retweets from 26971 tweets - youtu.be\n35060 retweets from 703 tweets - breaking911.com\n29745 retweets from 3965 tweets - nytimes.com\n29251 retweets from 2599 tweets - zerohedge.com\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import parse_qs, urlencode, urlparse\n",
    "\n",
    "def map_to_domain(url):\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    return parsed.netloc.replace(\"www.\", \"\")\n",
    "\n",
    "domain_url_map = transform_url_map(url_map, map_key=map_to_domain)\n",
    "print(\"Unique domains in the dataset: {:,}\".format(len(domain_url_map.keys())))\n",
    "print(\"Top domains in the dataset:\")\n",
    "\n",
    "top_urls_by_retweet_count(domain_url_map)"
   ]
  },
  {
   "source": [
    "## Top YouTube URLs in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Deleted URL params after normalizing Youtube URLs {'time_continue', 'start_radio', 'ab_channel', 'index', 'bsft_tv', 'utm_campaign', 'utm_medium', 't', 'ebc', 'feature', 'bsft_utid', 'pc', 'bsft_mid', 'bsft_lx', 'bsft_uid', 'amp;feature', 'html_redirect', 'search_query', 'from', 'bsft_ek', 'q', 'bsft_clkid', 'bsft_eid', 'view_as', 'reload', 'has_verified', 'list', 'bsft_link_id', 'noapp', 'utm_content', 'persist_app', 'd', 'authuser', 'utm_source', 'attr_tag', 'bsft_mime_type', 'pbjreload', 'app', 'redir_token', 'event', 'bsft_aaid', 'form', 'autoplay', 'lc', 'v', 'fbclid'}\n\nUnique Youtube URLs in the dataset: 5,530\nTop Youtube URLs in the dataset:\n7301 retweets from 433 tweets - https://youtu.be/w7vkbipeyz4\n4767 retweets from 336 tweets - https://youtu.be/96-bqaivopc\n3953 retweets from 30 tweets - https://youtu.be/byta1amljxy\n2504 retweets from 508 tweets - https://youtu.be/ztu5y5obwpk\n1605 retweets from 145 tweets - https://youtu.be/vgmpdnwunqs\n1404 retweets from 221 tweets - https://youtu.be/g9_sgyjnbko\n819 retweets from 775 tweets - https://youtu.be/ma8a2g6ttp0\n810 retweets from 4 tweets - https://youtu.be/v5jqbyaly0g\n742 retweets from 13 tweets - https://youtu.be/fveonzpdbiw\n722 retweets from 8 tweets - https://youtu.be/avy8echyd6i\n"
     ]
    }
   ],
   "source": [
    "deleted_url_params = set()\n",
    "\n",
    "def detect_youtube_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return \"youtu.be\" in parsed.netloc or \"youtube.com\" in parsed.netloc\n",
    "\n",
    "def normalize_youtube_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    parsed = parsed._replace()\n",
    "    query_params = parse_qs(parsed.query)\n",
    "    if (parsed.path == '/watch' and \"v\" in query_params):\n",
    "        updated_path = '/' + query_params[\"v\"][0]\n",
    "        del query_params[\"v\"]\n",
    "        parsed = parsed._replace(path=updated_path)\n",
    "    deleted_url_params.update(query_params.keys())\n",
    "    query_params = {}\n",
    "    updated_query = urlencode(query_params, doseq=True)\n",
    "    parsed = parsed._replace(scheme='https', netloc='youtu.be', query=updated_query)\n",
    "    return parsed.geturl()\n",
    "\n",
    "youtube_url_map = transform_url_map(\n",
    "    url_map, \n",
    "    filter_fn=detect_youtube_url,\n",
    "    map_key=normalize_youtube_url\n",
    ")\n",
    "\n",
    "print(\"Deleted URL params after normalizing Youtube URLs\", deleted_url_params)\n",
    "\n",
    "print()\n",
    "print(\"Unique Youtube URLs in the dataset: {:,}\".format(len(youtube_url_map.keys())))\n",
    "print(\"Top Youtube URLs in the dataset:\")\n",
    "top_urls_by_retweet_count(youtube_url_map)"
   ]
  },
  {
   "source": [
    "# Top Media in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 156311 json lines\n",
      "(64%): 100000 lines in ../data/14-nov/parsed_media.json processed (0.36994314193725586 sec)\n",
      "Done loading ../data/14-nov/parsed_media.json\n",
      "156311 lines in ../data/14-nov/parsed_media.json processed (0.5557003021240234 sec)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 78440 entries, 0 to 156298\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   media_url  78440 non-null  object\n",
      " 1   tweet_id   78440 non-null  object\n",
      " 2   media_id   78440 non-null  object\n",
      " 3   type       78440 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "media_df = load_media_df().drop_duplicates('media_id')\n",
    "media_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 78440 entries, 1324192339762520065 to 1324712263228497921\nData columns (total 47 columns):\n #   Column                      Non-Null Count  Dtype           \n---  ------                      --------------  -----           \n 0   media_url                   78440 non-null  object          \n 1   media_id                    78440 non-null  object          \n 2   type                        78440 non-null  object          \n 3   hashtags                    78434 non-null  object          \n 4   urls                        78434 non-null  object          \n 5   hasMedia                    78434 non-null  object          \n 6   quote_tweet                 31358 non-null  object          \n 7   retweet_count               78440 non-null  int32           \n 8   timestamp                   78434 non-null  object          \n 9   quote_count                 78440 non-null  int32           \n 10  user                        78434 non-null  object          \n 11  tokens                      78434 non-null  object          \n 12  voter fraud                 78440 non-null  Sparse[int64, 0]\n 13  #electionfraud              78440 non-null  Sparse[int64, 0]\n 14  ballot fraud                78440 non-null  Sparse[int64, 0]\n 15  #voterfraud                 78440 non-null  Sparse[int64, 0]\n 16  #stopthesteal               78440 non-null  Sparse[int64, 0]\n 17  stolen ballots              78440 non-null  Sparse[int64, 0]\n 18  election fraud              78440 non-null  Sparse[int64, 0]\n 19  ballot harvesting           78440 non-null  Sparse[int64, 0]\n 20  #electioninterference       78440 non-null  Sparse[int64, 0]\n 21  #cheatingdemocrats          78440 non-null  Sparse[int64, 0]\n 22  election interference       78440 non-null  Sparse[int64, 0]\n 23  #ballotfraud                78440 non-null  Sparse[int64, 0]\n 24  cheating democrats          78440 non-null  Sparse[int64, 0]\n 25  #electiontampering          78440 non-null  Sparse[int64, 0]\n 26  #ballotharvesting           78440 non-null  Sparse[int64, 0]\n 27  #voterfraudisreal           78440 non-null  Sparse[int64, 0]\n 28  destroyed ballots           78440 non-null  Sparse[int64, 0]\n 29  democrats cheat             78440 non-null  Sparse[int64, 0]\n 30  #stopvoterfraud             78440 non-null  Sparse[int64, 0]\n 31  #ballotvoterfraud           78440 non-null  Sparse[int64, 0]\n 32  election tampering          78440 non-null  Sparse[int64, 0]\n 33  #democratvoterfraud         78440 non-null  Sparse[int64, 0]\n 34  discarded ballots           78440 non-null  Sparse[int64, 0]\n 35  vote by mail fraud          78440 non-null  Sparse[int64, 0]\n 36  harvest ballot              78440 non-null  Sparse[int64, 0]\n 37  #gopvoterfraud              78440 non-null  Sparse[int64, 0]\n 38  #nomailinvoting             78440 non-null  Sparse[int64, 0]\n 39  #votebymailfraud            78440 non-null  Sparse[int64, 0]\n 40  #mailinvoterfraud           78440 non-null  Sparse[int64, 0]\n 41  pre-filled ballot           78440 non-null  Sparse[int64, 0]\n 42  hacked voting machine       78440 non-null  Sparse[int64, 0]\n 43  #ilhanomarballotharvesting  78440 non-null  Sparse[int64, 0]\n 44  #ilhanomarvoterfraud        78440 non-null  Sparse[int64, 0]\n 45  #hackedvotingmachines       78440 non-null  Sparse[int64, 0]\n 46  #discardedballots           78440 non-null  Sparse[int64, 0]\ndtypes: Sparse[int64, 0](35), int32(2), object(10)\nmemory usage: 13.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Preserve types when joining\n",
    "tweet_df_with_media = tweet_df[tweet_df['hasMedia'] == True]\n",
    "col_types = tweet_df_with_media.select_dtypes(include=['int', 'int32']).dtypes\n",
    "media_with_tweets_df = media_df.set_index('tweet_id').join(tweet_df_with_media, on='tweet_id')\n",
    "for col, col_type in col_types.iteritems():\n",
    "    media_with_tweets_df[col] = media_with_tweets_df[col].fillna(0).astype(col_type)\n",
    "\n",
    "media_with_tweets_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32145 retweets: http://pbs.twimg.com/media/EkM3ALlXsAAlZCR.jpg\n19627 retweets: http://pbs.twimg.com/media/EmHTacqXgAAL7Zo.jpg\n19546 retweets: http://pbs.twimg.com/media/EbwW3PeXQAAcjkD.jpg\n18538 retweets: http://pbs.twimg.com/media/EcIXpAAWAAEFPV5.jpg\n17252 retweets: http://pbs.twimg.com/amplify_video_thumb/1324105823845523456/img/2z47IGTqb_gZij3r.jpg\n13755 retweets: http://pbs.twimg.com/media/Ej56lVnWoAA0xEi.jpg\n13471 retweets: http://pbs.twimg.com/media/EmLU3OEWMAE-sV0.jpg\n13373 retweets: http://pbs.twimg.com/amplify_video_thumb/1301288746680307712/img/rjkmu6VgO-ZZCDaL.jpg\n12715 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1324811238682013704/pu/img/eb79SStIIBfEfMcN.jpg\n12434 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1324476554446069763/pu/img/z-GoJ3__Ctp5WZpa.jpg\n11384 retweets: http://pbs.twimg.com/media/EmF9szQXUAQ9-KL.jpg\n10782 retweets: http://pbs.twimg.com/media/Emg71EpXUAYup9Z.jpg\n10615 retweets: http://pbs.twimg.com/ext_tw_video_thumb/796043796618379264/pu/img/Z-R1rnH1yKdbQ3dT.jpg\n10014 retweets: http://pbs.twimg.com/amplify_video_thumb/1315394640842035217/img/bUdv0vkL1iyR6Ldx.jpg\n9653 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1324580630739537920/pu/img/DeFbx8xdKfOCI8CL.jpg\n9427 retweets: http://pbs.twimg.com/media/EmACdkyX0AQ7bDO.png\n8860 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1323969596764860417/pu/img/TVWQiBzRrl9GnIlJ.jpg\n8527 retweets: http://pbs.twimg.com/media/EmFpwPFXYAI_06X.jpg\n7808 retweets: http://pbs.twimg.com/amplify_video_thumb/1325130145477373952/img/43S-cxrjXV10qOKd.jpg\n7750 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1321854305163448320/pu/img/qCHkjziteY0frb5e.jpg\n6513 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1296864275408007173/pu/img/2iHQSyHKR_Cp31_0.jpg\n6210 retweets: http://pbs.twimg.com/media/EmLUPKCXYAEsPhT.jpg\n6210 retweets: http://pbs.twimg.com/media/EmLUO-iW0AIlJSt.jpg\n5937 retweets: http://pbs.twimg.com/media/EmPxCTeVkAEzSDD.jpg\n5778 retweets: http://pbs.twimg.com/ext_tw_video_thumb/1324060027301302273/pu/img/H3nPfo7k1XTPG3E0.jpg\n"
     ]
    }
   ],
   "source": [
    "def top_media_by_retweet_count(media_with_tweets_df, N = 25):\n",
    "    for media_id, media in media_with_tweets_df.nlargest(N, ['retweet_count']).iterrows():\n",
    "        retweet_count = media[\"retweet_count\"]\n",
    "        media_url = media[\"media_url\"]\n",
    "        print(\"{} retweets: {}\".format(retweet_count, media_url))\n",
    "\n",
    "top_media_by_retweet_count(media_with_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['http://pbs.twimg.com/amplify_video_thumb/1073399918365003776/img/ejp_AQNgJRIhv8Fj.jpg',\n",
       "       'http://pbs.twimg.com/amplify_video_thumb/1091352389825703936/img/dx1TMAkAtEJ3-ZKK.jpg',\n",
       "       'http://pbs.twimg.com/amplify_video_thumb/1139265519721336832/img/9RoODfrkiRyrWNWi.jpg',\n",
       "       ..., 'http://pbs.twimg.com/tweet_video_thumb/EmzocgMXIAA5brg.jpg',\n",
       "       'http://pbs.twimg.com/tweet_video_thumb/Emzoj-bUwAAXZ3H.jpg',\n",
       "       'http://pbs.twimg.com/tweet_video_thumb/EmzpOemVkAEmB05.jpg'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "np.unique(media_with_tweets_df['media_url'])"
   ]
  },
  {
   "source": [
    "## Export to JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_sets(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "\n",
    "    return obj\n",
    "\n",
    "with open(\"./data_export/url_stats/youtube_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(youtube_url_map, f, sort_keys=True, indent=2, default=serialize_sets)\n",
    "\n",
    "with open(\"./data_export/url_stats/domains.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(domain_url_map, f, sort_keys=True, indent=2, default=serialize_sets)\n",
    "\n",
    "with open(\"./data_export/url_stats/all_urls.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(url_map, f, sort_keys=True, indent=2, default=serialize_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}